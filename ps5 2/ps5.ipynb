{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd5fdf63-0a79-4490-aa8f-3669ba1a7f58",
   "metadata": {},
   "source": [
    "# Problem Set 5: Neural Networks\n",
    "\n",
    "**Release Date:** 1 April 2025\n",
    "\n",
    "**Due Date:** 19 April 2025"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b3fadaa-a686-498a-b9c0-d56a6c8b591f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the real world, while fundamentals are welcomed and appreciated, implementing algorithms from scratch is time consuming, especially when it comes to Deep Learning (DL) models like neural networks with many layers. Backpropagating manually or by hand is often tedious and erroneous. Which is why, it is absolutely critical to learn **at least one** Machine Learning library, either to get jobs or build projects in this field. As such, in *Problem Set 5*, we will introduce you to **PyTorch**.\n",
    "\n",
    "![PyTorch](images/logo.png)\n",
    "\n",
    "`PyTorch` is one of the most widely-used DL library. It offers a very Pythonic API to build layers and compose them together. In fact, data processing is also made easy using the multitude of tools and wrappers that are at your disposal. Of course, there are other popular libraries such as `TensorFlow`, but they require you to understand \"computation graphs\", which we feel makes it less accessible for beginners. Hence, we decided to use PyTorch for CS2109S. \n",
    "\n",
    "In *Problem Set 5*, we will attempt to help you learn the `PyTorch` API by having you build a simple deep neural network and training it locally on your system via backpropagation and stochastic gradient descent. You will also learn how to build data processing pipelines to prepare your data before ingestion into your model(s). Subsequently, you will be building a __Convolutional Neural Network__ (CNN/ConvNet) and training it on two datasets, *MNIST* and *CIFAR-10*. You'll also learn how to build __data augmentation pipelines__ to enhance your dataset. We will then learn the details of RNN, as well as their applications to sequential data. Lastly, we will use an RNN to model and predict patterns in time-series data, specifically using sine wave."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e81322e",
   "metadata": {},
   "source": [
    "### Required Files\n",
    "* ps5.ipynb\n",
    "* data/\n",
    "    * review_train.csv\n",
    "    * review_test.csv\n",
    "\n",
    "### Plagiarism Policy\n",
    "\n",
    "Please refer to our [Course Policies](https://canvas.nus.edu.sg/courses/62323/pages/course-policies)\n",
    "\n",
    "_Honour Code: Note that plagiarism will not be condoned! You may discuss with your classmates and check the internet for references, but you MUST NOT submit any code/report/explanation that is copied directly from other sources!_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42025bf4",
   "metadata": {},
   "source": [
    "### Post-Problem Set Survey\n",
    "Your feedback is important to us! After completing Problem Set 5, please take a moment to share your thoughts by filling out this [survey](https://coursemology.org/courses/2951/surveys/2566)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e8fb6f",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following lines of code import packages and functions that are necessary\n",
    "for the following tasks.\n",
    "\n",
    "As a reminder, please **do not** modify the following lines of code by adding,\n",
    "removing or modifying the specified imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a167fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL FIRST\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from numpy import allclose, isclose\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13434c4e",
   "metadata": {},
   "source": [
    "# 1 Tensors in PyTorch\n",
    "\n",
    "### Concept 1.1 - What are Tensors?\n",
    "\n",
    "In your linear algebra class, you have learnt about vectors. In essence, they are 1-dimensional (1D) serial arrays (like `[1, 2, 3, 4, 23, 18]`) containing a column (or row) of information. You have also learned about matrices – they are \"rectangles\" (i.e., 2D arrays) that also capture elements.\n",
    "\n",
    "**Tensors** generalise the concept of matrices: they are $n$-dimensional arrays that contain or represent information. In *PyTorch*, everything is defined as a `tensor`. It's analogous to `np.array(...)` from *NumPy*. A `tensor` object in *PyTorch* looks like this:\n",
    "\n",
    "![PyTorch](images/tensors.png)\n",
    "\n",
    "\n",
    "---\n",
    "The following are some mappings of useful functions between Numpy and Pytorch, in fact, they are so similar that there is a function `torch.from_numpy(ndarray)` which transforms a NumPy array into a PyTorch tensor! The main difference in the functions in the table below is that NumPy and PyTorch functions takes as input and gives as output either `numpy` array or `torch` tensors, respectively. PyTorch tensors also have an additional functionality for GPU acceleration. Refer to this [website](https://pytorch-for-numpy-users.wkentaro.com/) for more information.\n",
    "\n",
    "![PyTorch](images/numpy_pytorch_diff.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cfa263c",
   "metadata": {},
   "source": [
    "### Demo 1.1 - Tensor functions\n",
    "\n",
    "Notice that tensors have a `.grad` attribute. This is used for automatic gradient computation.  \n",
    "To create tensors, you can use the `torch.tensor(...)` constructor:  \n",
    "\n",
    "A 0-dimensional tensor: `torch.tensor(5.0)`  \n",
    "A 1-dimensional tensor: `torch.tensor([1.0, 2.0, 3.0])`  \n",
    "A 2-dimensional tensor: `torch.tensor([[.4, .3], [.1, .2]])`  \n",
    "\n",
    "If automatic gradient computation is required, then the equivalent constructors will be:  \n",
    "`torch.tensor(5.0, requires_grad=True)`  \n",
    "`torch.tensor([1.0, 2.0, 3.0], requires_grad=True)`  \n",
    "`torch.tensor([[.4, .3], [.1, .2]], requires_grad=True)`  \n",
    "\n",
    "We can call `detach()` on these tensors to stop them from being traced for gradient computation, returning us the tensors without `requires_grad=True`.\n",
    "\n",
    "We can call `item()` on our tensors to return the value of our tensor as a standard Python number:\n",
    "\n",
    "`>>> torch.tensor([1.0]).item()`\n",
    "\n",
    "`1.0`\n",
    "\n",
    "The following code block shows how we can make use of all these functions introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with requires_grad set to True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Compute the gradient of a simple expression using backward\n",
    "y = x**2 + 2 * x\n",
    "y.backward()\n",
    "\n",
    "# Print the derivative value of y i.e dy/dx = 2x + 2 = 6.0.\n",
    "print(\"Gradient of y with respect to x:\", x.grad)\n",
    "\n",
    "# Detach the gradient of x\n",
    "x = x.detach()\n",
    "\n",
    "# Print the gradient of x after detachment\n",
    "print(\"Gradient of x after detachment:\", x.grad)\n",
    "\n",
    "# Extract the scalar value of a tensor as a Python number\n",
    "x_value = x.item()\n",
    "print(\"Value of x as a Python number:\", x_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f55719",
   "metadata": {},
   "source": [
    "### Demo 1.2 - Working with Tensors\n",
    "\n",
    "Here, we use `torch.linspace` to create a `torch.tensor`. In PyTorch (and Machine Learning in general) tensors form the basis of all operations.\n",
    "\n",
    "We then make use of the built-in *PyTorch* function `torch.sin` to create the corresponding y-values of a sine function, and plot the points using *Matplotlib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40456e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration: You just need to run this cell without editing.\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 1000) # Task 1.1: What is torch.linspace?\n",
    "y_true = torch.sin(x)\n",
    "\n",
    "plt.plot(x, y_true, linestyle='solid', label='sin(x)')\n",
    "plt.axis('equal')\n",
    "plt.title('Original function to fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7642e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to explore what the FIRST 10 VALUES of x has been assigned to.\n",
    "# By default, each cell will always print the output of the last expression in the cell\n",
    "# You can explore what x is by modifying the expression e.g. x.max(), x.shape\n",
    "x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "592826c4",
   "metadata": {},
   "source": [
    "### Task 1.1 - What is `torch.linspace`?\n",
    "\n",
    "From the example above, answer the following questions:\n",
    "\n",
    "1. What does `x = torch.linspace(-math.pi, math.pi, 1000)` do?  \n",
    "2. How many values are stored in `x`?  \n",
    "3. What are the minimum and maximum values in `x`?  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81f08a1",
   "metadata": {},
   "source": [
    "### Demo 1.3 - Using Tensors for linear regression\n",
    "\n",
    "For this example, we fit a **degree 3 polynomial** to the sine function, using a learning rate of `1e-6` and `5000` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2022a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration: You just need to run this cell without editing.\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Initialize weights to 0\n",
    "a = torch.tensor(0.)\n",
    "b = torch.tensor(0.)\n",
    "c = torch.tensor(0.)\n",
    "d = torch.tensor(0.)\n",
    "\n",
    "print('iter', 'loss', '\\n----', '----', sep='\\t')\n",
    "for t in range(1, 5001): # 5000 iterations\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x**2 + d * x**3\n",
    "\n",
    "    # Compute MSE loss\n",
    "    loss = torch.mean(torch.square(y_pred - y_true))\n",
    "    if t % 1000 == 0:\n",
    "        print(t, loss.item(), sep='\\t')\n",
    "\n",
    "    # Backpropagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y_true) / y_pred.shape[0]\n",
    "    \n",
    "    # Compute gradients of a, b, c, d with respect to loss\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "# print fitted polynomial\n",
    "equation = f'{a:.5f} + {b:.5f} x + {c:.5f} x^2 + {d:.5f} x^3'\n",
    "\n",
    "y_pred = a + b * x + c * x**2 + d * x**3\n",
    "plt.plot(x, y_true, linestyle='solid', label='sin(x)')\n",
    "plt.plot(x, y_pred, linestyle='dashed', label=f'{equation}')\n",
    "plt.axis('equal')\n",
    "plt.title('3rd degree poly fitted to sine (MSE loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d5270f",
   "metadata": {},
   "source": [
    "### Demo 1.4 - Using autograd to automatically compute gradients\n",
    "\n",
    "In the previous example, we explicitly computed the gradient for Mean Squared Error (MSE):  \n",
    "`grad_y_pred = 2.0 * (y_pred - y_true) / y_pred.shape[0]`\n",
    "\n",
    "In the next example, we will use PyTorch's autograd functionality to help us compute the gradient for **Mean Absolute Error (MAE)**.  \n",
    "In order to compute the gradients, we will use the `.backward()` method of *PyTorch* tensors.\n",
    "\n",
    "Once again, we fit a **degree 3 polynomial** to the sine function, using a learning rate of `1e-6` and `5000` iterations.  \n",
    "This time, we will use MAE instead of MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration: You just need to run this cell without editing.\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Initialize weights to 0\n",
    "a = torch.tensor(0., requires_grad=True)\n",
    "b = torch.tensor(0., requires_grad=True)\n",
    "c = torch.tensor(0., requires_grad=True)\n",
    "d = torch.tensor(0., requires_grad=True)\n",
    "\n",
    "print('iter', 'loss', '\\n----', '----', sep='\\t')\n",
    "for t in range(1, 5001):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute MAE loss\n",
    "    loss = torch.mean(torch.abs(y_pred - y_true))\n",
    "    if t % 1000 == 0:\n",
    "        print(t, loss.item(), sep='\\t')\n",
    "\n",
    "    # Automatically compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        a.grad.zero_() # reset gradients !important\n",
    "        b.grad.zero_() # reset gradients !important\n",
    "        c.grad.zero_() # reset gradients !important\n",
    "        d.grad.zero_() # reset gradients !important\n",
    "        # What happens if you don't reset the gradients?\n",
    "\n",
    "# print fitted polynomial\n",
    "equation = f'{a:.5f} + {b:.5f} x + {c:.5f} x^2 + {d:.5f} x^3'\n",
    "\n",
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "plt.plot(x, y_true, linestyle='solid', label='sin(x)')\n",
    "plt.plot(x, y_pred.detach().numpy(), linestyle='dashed', label=f'{equation}')\n",
    "plt.axis('equal')\n",
    "plt.title('3rd degree poly fitted to sine (MAE loss)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01e7df2",
   "metadata": {},
   "source": [
    "### Task 1.2 - Polyfit model\n",
    "\n",
    "We have demonstrated how to fit a degree-3 polynomial to a set of `x` and `y` points (following the sine curve), using two different types of loss functions (MSE and MAE).  \n",
    "\n",
    "Now, your task is to write a function `polyfit` that takes in some arbitrary set of points. You are only allowed to use **ONE** loop for the backpropagation and weights update. You are **NOT** allowed to use a loop to raise the features to their respective powers.\n",
    "1. `x`, corresponding x-values,  \n",
    "2. `y`, corresponding true y-values,  \n",
    "3. `loss_fn` to compute the loss, given the true `y` and predicted `y`,  \n",
    "4. `n` representing the $n$-degree polynomial, and \n",
    "5. `lr` learning rate, and  \n",
    "6. `n_iter` for the number of times to iterate.  \n",
    "\n",
    "Return the 1D tensor containing the coefficients of the $n$-degree polynomial , after fitting the model.  \n",
    "The coefficients should be arranged in ascending powers of $x$.\n",
    "\n",
    "For example,\n",
    "```\n",
    ">>> y = torch.sin(x)\n",
    ">>> mse = lambda y_true, y_pred: torch.mean(torch.square(y_pred - y_true))\n",
    ">>> mae = lambda y_true, y_pred: torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    ">>> polyfit(x, y, mse, 3, 1e-3, 5000)\n",
    "tensor([-4.2270e-09,  8.5167e-01,  1.2131e-08, -9.2587e-02], requires_grad=True))\n",
    "\n",
    ">>> polyfit(x, y, mae, 3, 1e-3, 5000)\n",
    "tensor([-9.6776e-07,  8.7905e-01, -2.4784e-06, -9.8377e-02], requires_grad=True))\n",
    "```\n",
    "\n",
    "*Note: For this regression problem, initialize your weights to 0.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polyfit(x: torch.Tensor, y: torch.Tensor, loss_fn: Callable, n: int, lr: float, n_iter: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        x : A tensor of shape (1, n)\n",
    "        y : A tensor of shape (1, n)\n",
    "        loss_fn : Function to measure loss\n",
    "        n : The nth-degree polynomial\n",
    "        lr : Learning rate\n",
    "        n_iter : The number of iterations of gradient descent\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Near-optimal coefficients of the nth-degree polynomial as a tensor of shape (1, n+1) after `n_iter` epochs.\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 1000)\n",
    "\n",
    "# Original true values\n",
    "y = torch.sin(x)\n",
    "plt.plot(x, y, linestyle='solid', label='sin(x)')\n",
    "\n",
    "# MSE\n",
    "mse = lambda y_true, y_pred: torch.mean(torch.square(y_pred - y_true))\n",
    "a, b, c, d = polyfit(x, y, mse, 3, 1e-6, 5000)\n",
    "y_pred_mse = a + b * x + c * x ** 2 + d * x ** 3\n",
    "plt.plot(x, y_pred_mse.detach().numpy(), linestyle='dashed', label=f'mse')\n",
    "\n",
    "# MAE\n",
    "mae = lambda y_true, y_pred: torch.mean(torch.abs(y_pred - y_true))\n",
    "a, b, c, d = polyfit(x, y, mae, 3, 1e-3, 5000)\n",
    "y_pred_mae = a + b * x + c * x ** 2 + d * x ** 3\n",
    "plt.plot(x, y_pred_mae.detach().numpy(), linestyle='dashed', label=f'mae')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Comparison of different fits')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "x = torch.linspace(-math.pi, math.pi, 10)\n",
    "y = torch.sin(x)\n",
    "\n",
    "def mse(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    assert y_true.shape == y_pred.shape, f\"Your ground truth and predicted values need to have the same shape {y_true.shape} vs {y_pred.shape}\"\n",
    "    return torch.mean(torch.square(y_pred - y_true))\n",
    "\n",
    "def mae(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    assert y_true.shape == y_pred.shape, f\"Your ground truth and predicted values need to have the same shape {y_true.shape} vs {y_pred.shape}\"\n",
    "    return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "test1 = polyfit(x, x, mse, 1, 1e-1, 100).tolist()\n",
    "test2 = polyfit(x, x**2, mse, 2, 1e-2, 2000).tolist()\n",
    "test3 = polyfit(x, y, mse, 3, 1e-3, 5000).tolist()\n",
    "test4 = polyfit(x, y, mae, 3, 1e-3, 5000).tolist()\n",
    "\n",
    "assert allclose(test1, [0.0, 1.0], atol=1e-6)\n",
    "assert allclose(test2, [0.0, 0.0, 1.0], atol=1e-5)\n",
    "assert allclose(test3, [0.0, 0.81909, 0.0, -0.08469], atol=1e-3)\n",
    "assert allclose(test4, [0.0, 0.83506, 0.0, -0.08974], atol=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76cbf142",
   "metadata": {},
   "source": [
    "### Task 1.3 - Observations on different model configurations\n",
    "\n",
    "Run `polyfit` on these model configurations and explain your observations for <b>ALL</b> four configurations. Refer to the learning rate and degree of the polynomial when making observations regarding how well the model converges if at all.\n",
    "\n",
    "1. `polyfit(x, y, mse, 3, 1e-6, 5000)`\n",
    "2. `polyfit(x, y, mse, 3, 1e6, 5000)`\n",
    "3. `polyfit(x, y, mse, 1, 1e-3, 5000)`\n",
    "4. `polyfit(x, y, mse, 6, 1e-3, 5000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38666754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may use this cell to run your observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dda5761",
   "metadata": {},
   "source": [
    "---\n",
    "# 2 Computing gradients for arbitrary graphs\n",
    "\n",
    "Recall the neural network for `y = |x-1|` from the lecture. We are going to implement forward propagation as mentioned during lecture. This forward pass is the act of feeding data into our input layer, which will then be passed to and processed by the hidden layers according to the different activation functions specific to each perceptron. After passing through all the hidden layers, our neural network will generate an output, $\\hat{y}$, that is hopefully meaningful to our problem at hand.\n",
    "\n",
    "![PyTorch](images/toy_nn.png)\n",
    "\n",
    "### Task 2.1 - Forward pass\n",
    "\n",
    "In this task, you are required implement the function `forward_pass` that takes in 4 arguments:  \n",
    "1. `x`, the input values (not including bias)\n",
    "2. `w0`, (2x2) weights of the hidden layer\n",
    "3. `w1`, (3x1) weights of the output layer\n",
    "4. `activation_fn`, the activation function of the hidden layer.\n",
    "\n",
    "*Note: As in the lecture, there will be no activation for the output layer (i.e. the activation function of the output layer is the identity function `lambda x: x`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "# y = torch.abs(x-1)\n",
    "def forward_pass(x: torch.Tensor, w0: torch.Tensor, w1: torch.Tensor, activation_fn: Callable):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "# Exact weights\n",
    "w0 = torch.tensor([[-1., 1.], [1., -1.]], requires_grad=True)\n",
    "w1 = torch.tensor([[0.], [1.], [1.]], requires_grad=True)\n",
    "\n",
    "# Performing a forward pass on exact solution for weights will give us the correct y values\n",
    "x_sample = torch.linspace(-2, 2, 5).reshape(-1, 1)\n",
    "forward_pass(x_sample, w0, w1, torch.relu) # tensor([[3.], [2.], [1.], [0.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ff3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "w0 = torch.tensor([[-1., 1.], [1., -1.]], requires_grad=True)\n",
    "w1 = torch.tensor([[0.], [1.], [1.]], requires_grad=True)\n",
    "\n",
    "output0 = forward_pass(torch.linspace(0,1,50).reshape(-1, 1), w0, w1, torch.relu)\n",
    "x_sample = torch.linspace(-2, 2, 5).reshape(-1, 1)\n",
    "test1 = forward_pass(x_sample, w0, w1, torch.relu).tolist()\n",
    "output1 = [[3.], [2.], [1.], [0.], [1.]]\n",
    "\n",
    "assert output0.shape == torch.Size([50, 1])\n",
    "assert test1 == output1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91843aa0",
   "metadata": {},
   "source": [
    "### Task 2.2 - Backward propagation\n",
    "\n",
    "In this task, you will start with random weights for `w0` and `w1`, and iteratively perform forward passes and backward propagation multiple times to converge on a solution.\n",
    "\n",
    "Submit your values of `w0`, `w1`, and `loss` value onto Coursemology. Your `loss` value should be less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ff398",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "y = torch.abs(x-1)\n",
    "torch.manual_seed(1) # Set seed to some fixed value\n",
    "w0 = torch.randn(2, 2, requires_grad=True)\n",
    "w1 = torch.randn(3, 1, requires_grad=True)\n",
    "learning_rate = 0.023\n",
    "print('iter', 'loss', '\\n----', '----', sep='\\t')\n",
    "for t in range(1, 10001):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = forward_pass(x, w0, w1, torch.relu)\n",
    "\n",
    "    loss = torch.mean(torch.square(y - y_pred))\n",
    "    loss.backward()\n",
    "\n",
    "    if t % 1000 == 0:\n",
    "        print(t, loss.item(), sep='\\t')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \"\"\"\n",
    "        Update weights and then reset the gradients to zero here\n",
    "        \"\"\"\n",
    "        \n",
    "y_pred = forward_pass(x, w0, w1, torch.relu)\n",
    "loss = torch.mean(torch.square(y - y_pred)).item()\n",
    "print(\"--- w0 ---\", w0, sep='\\n')\n",
    "print(\"--- w1 ---\", w1, sep='\\n')\n",
    "plt.plot(x, y, linestyle='solid', label='|x-1|')\n",
    "plt.plot(x, y_pred.detach().numpy(), linestyle='dashed', label='perceptron')\n",
    "plt.axis('equal')\n",
    "plt.title('Fit NN on abs function')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "paste your w0, w1 and loss values here\n",
    "w0 = ...\n",
    "w1 = ...\n",
    "loss = ...\n",
    "\"\"\"\n",
    "\"\"\" YOUR CODE HERE \"\"\"\n",
    "raise NotImplementedError\n",
    "\"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10337afb",
   "metadata": {},
   "source": [
    "### Task 2.3 - Different random seeds\n",
    "\n",
    "Try to fit the model on different initial random weight values by adjusting the random seed. \n",
    "<br/>\n",
    "What is the impact of a random seed? How should we compare different neural network models given your observation to ensure fairness?\n",
    "\n",
    "Submit your observations and conclusion on Coursemology.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da8e3c54",
   "metadata": {},
   "source": [
    "# 3 Neural Networks (using PyTorch layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "763f853c",
   "metadata": {},
   "source": [
    "### Demo 3.1 - nn.Module\n",
    "\n",
    "The `nn.Module` class is an interface that houses two main methods: `__init__`, where we instantiate our layers and activation functions, and `forward`, that performs the forward pass.\n",
    "\n",
    "To create our own neural network, we will inherit from the nn.Module parent class and call `super().__init__()` from within our constructor to create our module. Next, we will implement the `forward` function within our class so we can call it from our module to perform the forward pass. \n",
    "\n",
    "In this example, we define a custom LinearLayer class that inherits from nn.Module. The __init__ method initializes the weight and bias parameters as nn.Parameter objects, which are special types of tensors that require gradients to be computed during the backward pass.\n",
    "\n",
    "The forward method defines the forward pass of the linear layer. It takes a tensor x as input and computes the matrix multiplication of x and self.weight using the torch.matmul function, and then adds self.bias.\n",
    "\n",
    "We also created our own activation function which uses `torch.sin` by inheriting from nn.Module.\n",
    "\n",
    "Finally, in our Model, we can combine our own LinearLayers together with our SineActivation to process our input data using the forward function. In later sections, you will see how we can train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b196ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear layer using nn.Module\n",
    "class LinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer as a subclass of `nn.Module`.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "    \n",
    "class SineActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Sine activation layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network created using `LinearLayer` and `SineActivation`.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = LinearLayer(input_size, hidden_size)\n",
    "        self.act = SineActivation()\n",
    "        self.l2 = LinearLayer(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.l1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "num_classes = 1\n",
    "\n",
    "model = Model(input_size, hidden_size, num_classes)\n",
    "\n",
    "x = torch.tensor([[1.0]])\n",
    "output = model(x)\n",
    "print(\"Original value: \", x)\n",
    "print(\"Value after being processed by Model: \", output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84a88b6a",
   "metadata": {},
   "source": [
    "_Extra: We can also define a `backward` function to perform backpropagation which will not be required in this problem set._\n",
    "\n",
    "In this trivial example, the Squared module takes an input x and returns x**2. The backward method calculates the gradient of the output with respect to the input, based on the gradients of the output grad_output.\n",
    "\n",
    "We can define the backward function for functions that are not fully differentiable that we still wish to use in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc40790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squared(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that returns x**2.\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.x = x\n",
    "        return x**2\n",
    "\n",
    "    def backward(self, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        grad_input = 2 * self.x * grad_output\n",
    "        return grad_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b0474e",
   "metadata": {},
   "source": [
    "### Demo 3.2 - Activation Functions\n",
    "\n",
    "Pytorch also provides built-in activation functions. To help you understand more about activation functions, we have included some examples of activation functions introduced in the lecture, namely Sigmoid, Tanh, and ReLu. \n",
    "\n",
    "![PyTorch](images/img_activation_fns.png)\n",
    "\n",
    "Activation functions introduces non-linearity into the output of a neuron, allowing the NN to learn non-linear functions. Without non-linearity, our entire network will effectively become a linear model with only one layer, preventing us from modelling complex representations based on our inputs.\n",
    "\n",
    "Sigmoid, Tanh and ReLU are three examples of such activation functions introduced during lecture and the code block below shows how they map input to output values.\n",
    "\n",
    "The choice of activation function for the hidden layers and the output layer depends on the problem you're trying to solve.\n",
    "\n",
    "####  For the hidden layers, there are several commonly used activation functions:\n",
    "\n",
    "ReLU (Rectified Linear Unit): ReLU is a popular activation function that is widely used in deep learning models. It maps non-positive inputs to 0 and positive inputs to their original value. It is mainly used in hidden layers because it is fast to compute, has sparse activations, and helps to mitigate the vanishing gradient problem, where the gradients can become very small and cause the model to learn slowly.\n",
    "\n",
    "Tanh (Hyperbolic Tangent): Tanh is a activation function that maps input values to the range [-1, 1]. It is similar to Sigmoid, but instead of producing output values in the range [0, 1], it produces output values in the range [-1, 1]. Tanh is useful for solving problems where you want the activations to be centered around zero, such as in recurrent neural networks.\n",
    "\n",
    "Sigmoid: Sigmoid maps its input values to the range [0, 1]. It is less commonly used in hidden layers because it has a relatively slow convergence rate and can introduce saturation, where the output values become very small or very large, which can make it difficult for the gradients to flow through the model.\n",
    "\n",
    "#### For the output layer, the choice of activation function depends on the problem you're trying to solve. Here are some common choices:\n",
    "\n",
    "Sigmoid: The Sigmoid activation function maps input values to the range [0, 1]. It is commonly used for binary classification problems where the network produces a probability of one of two classes. In this case, the Sigmoid activation maps the output to a probability distribution over the two classes.\n",
    "\n",
    "Softmax: The Softmax activation function is a generalization of the Sigmoid activation that maps input values to a probability distribution over multiple classes. It is commonly used for multiclass classification problems. The Softmax activation function is used to convert the raw scores produced by the network into a probability distribution over the classes.\n",
    "\n",
    "Linear: For regression problems, the linear activation function is often used because it just maps the input values to the output values without any change.\n",
    "\n",
    "In summary, ReLU is a common choice for hidden layers, and the choice of activation function for the output layer depends on the problem you're trying to solve (binary classification, multiclass classification, or regression).\n",
    "\n",
    "---\n",
    "\n",
    "_Extra (Vanishing Gradient Problem):_\n",
    "\n",
    "_Below is an image of the derivatives of the Sigmoid, Tanh and ReLU function. We can see that the derivatives for both Sigmoid and Tanh tend to zero when the inputs are largely positive or negative, while derivative for ReLU is zero only when the inputs are non-positive. In our neural network, gradients are calculated through backpropagation using chain rule and the derivatives of each layer are multiplied down the network. The gradient is more likely to decrease exponentially as we propagate down to the initial layers if we use Sigmoid and Tanh as compared to ReLU, leading to the vanishing gradient problem._\n",
    "\n",
    "![PyTorch](images/img_activation_fns_der.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = torch.linspace(-2, 2, 100)\n",
    "sigmoid_output = nn.Sigmoid()(x_sample).detach().numpy()\n",
    "tanh_output = nn.Tanh()(x_sample).detach().numpy()\n",
    "relu_output = nn.ReLU()(x_sample).detach().numpy()\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_figwidth(6)\n",
    "f.set_figheight(6)\n",
    "plt.xlabel('x - axis')\n",
    "plt.ylabel('y - axis')\n",
    "plt.title(\"Input: 100 x-values between -2 to 2 \\n\\n Output: Corresponding y-values after passed through each activation function\\n\", fontsize=16)\n",
    "plt.axvline(x=0, color='r', linestyle='dashed')\n",
    "plt.axhline(y=0, color='r', linestyle='dashed')\n",
    "plt.plot(x_sample, sigmoid_output)\n",
    "plt.plot(x_sample, tanh_output)\n",
    "plt.plot(x_sample, relu_output)\n",
    "plt.legend([\"\",\"\",\"Sigmoid Output\", \"Tanh Output\", \"ReLU Output\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3baf89e",
   "metadata": {},
   "source": [
    "### Task 3.1 - Forward pass (NN)\n",
    "\n",
    "In part 2, you manually created the Linear layers and explicitly specified weights and biases for the forward pass to connect every input neuron to every output neuron which will be extremely tedious for larger networks. \n",
    "\n",
    "In this task, you will be using `nn.Linear(in_dimensions, out_dimensions)` provided by pytorch which abstracts all these details away. `nn.Linear` represents a fully connected layer with bias automatically included. We can also choose to remove the bias column by simply calling `nn.Linear(in_dimensions, out_dimensions, bias=False)` instead.\n",
    "\n",
    "We inherit from PyTorch's `nn.Module` class to build the model from the previous task `y = |x-1|` from the lecture.  \n",
    "\n",
    "![PyTorch](images/toy_nn.png)\n",
    "\n",
    "Pytorch is widely used in machine learning due to the ease of being able to combine many different types of layers and activation functions to create neural networks. This task should allow you to appreciate how easily we can build neural networks using PyTorch.  \n",
    "\n",
    "The model has been built for you in `__init__`. You need to implement the `forward` method, making use of the layers `self.l1`, `self.l2`, and the activation function `self.relu`. You need to combine the linear layers AND the activation function in the forward pass function!\n",
    "\n",
    "_Extra: PyTorch has many other layers implemented for various model architectures.  \n",
    "You can read more in the glossary as well as in the docs: https://pytorch.org/docs/stable/nn.html  \n",
    "For now, we will only be using fully connected `nn.Linear` layers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNet(nn.Module):\n",
    "    def __init__(self): # set the arguments you'd need\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(1, 2) # bias included by default\n",
    "        self.l2 = nn.Linear(2, 1) # bias included by default\n",
    "        self.relu = nn.ReLU()\n",
    " \n",
    "    # Task 3.1: Forward pass\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass to process input through two linear layers and ReLU activation function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : A tensor of of shape (n, 1) where n is the number of training instances\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor of shape (n, 1)\n",
    "        '''\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "x_sample = torch.linspace(-2, 2, 5).reshape(-1, 1)\n",
    "\n",
    "model = MyFirstNeuralNet()\n",
    "\n",
    "state_dict = OrderedDict([\n",
    "    ('l1.weight', torch.tensor([[1.],[-1.]])),\n",
    "    ('l1.bias',   torch.tensor([-1., 1.])),\n",
    "    ('l2.weight', torch.tensor([[1., 1.]])),\n",
    "    ('l2.bias',   torch.tensor([0.]))\n",
    "])\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "student1 = model.forward(x_sample).detach().numpy()\n",
    "output1 = [[3.], [2.], [1.], [0.], [1.]]\n",
    "\n",
    "assert allclose(student1, output1, atol=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd2e0500",
   "metadata": {},
   "source": [
    "### Demo 3.3 - Optimisers in PyTorch\n",
    "\n",
    "Optimizers in PyTorch are used to update the parameters of a model during training. They do this by computing the gradients of the model's parameters with respect to the loss function, and then using these gradients to update the parameters in a way that minimizes the loss. \n",
    "\n",
    "In the following code example, we will simply demo a few basic functionalities of optimisers. Only in 3.1.4 Demo will you see an actual optimizer at work to train a Neural Net.\n",
    "\n",
    "We first create a tensor x with requires_grad set to True. Next, we define our loss function to be the simple equation y = x ** 2 + 2 * x. Next, we define an optimiser (in this case, Stochastic Gradient Descent, SGD) and pass it our tensor x as a parameter to optimise. After updating the gradient stored in x using `backward()`, we will call the `step()` function to let the optimiser update x. We will then set the gradient of our tensor x back to zero using `zero_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "#Loss function\n",
    "y = x ** 2 + 2 * x\n",
    "\n",
    "# Define an optimizer, pass it our tensor x to update\n",
    "optimiser = torch.optim.SGD([x], lr=0.1)\n",
    "\n",
    "# Perform backpropagation\n",
    "y.backward()\n",
    "\n",
    "print(\"Value of x before it is updated by optimiser: \", x)\n",
    "print(\"Gradient stored in x after backpropagation: \", x.grad)\n",
    "\n",
    "# Call the step function on the optimizer to update weight\n",
    "optimiser.step()\n",
    "\n",
    "#Weight update, x = x - lr * x.grad = 1.0 - 0.1 * 4.0 = 0.60\n",
    "print(\"Value of x after it is updated by optimiser: \", x)\n",
    "\n",
    "# Set gradient of weight to zero\n",
    "optimiser.zero_grad()\n",
    "print(\"Gradient stored in x after zero_grad is called: \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1efa2827",
   "metadata": {},
   "source": [
    "### Demo 3.4 - Training Your First Neural Net\n",
    "\n",
    "Now, let's make use of an optimiser to train our neural network in Task 3.1!\n",
    "\n",
    "Take note, if you make changes to your model (e.g. fix any bugs in your forward pass), then you will have to re-run your previous cell to update the model definition.\n",
    "\n",
    "In the example below, we are applying what we have learnt in the above section about optimisers to train our neural network.\n",
    "\n",
    "We will using `torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0)` as the optimiser. This SGD optimiser will implement stochastic gradient descent for us. As mentioned previously, `optimiser.zero_grad()` will set all the gradients to zero to prevent accumulation of all the previous old gradients we have calculated using backpropagation. `optimiser.step()` causes our optimiser to update the model weights based on the gradients of our parameters.\n",
    "\n",
    "We can see clearly from our example below that we are calling `optimiser.zero_grad()` at the start of the loop so we can clear the gradient from the previous iteration of backpropagation. Then after we compute the loss in the current iteration using our loss function and model predictions, y_pred, we will call `loss.backward()` to let pytorch carry out the backpropagation for us. After backpropagation, gradients for each of our parameters will be computed for us to update our model weights using `optimiser.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "model = MyFirstNeuralNet()\n",
    "# the optimizer controls the learning rate\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x = torch.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "y = torch.abs(x-1)\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs+1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "y_pred = model(x)\n",
    "plt.plot(x, y, linestyle='solid', label='|x-1|')\n",
    "plt.plot(x, y_pred.detach().numpy(), linestyle='dashed', label='perceptron')\n",
    "plt.axis('equal')\n",
    "plt.title('Fit NN on y=|x-1| function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd0a78aa",
   "metadata": {},
   "source": [
    "### Concept 3.1 - Save and load models\n",
    "\n",
    "Your model weights are stored within the model itself.  \n",
    "You may save/load the model weights:\n",
    "```\n",
    "torch.save(model.state_dict(), \"path/to/model_state_dict\")\n",
    "\n",
    "model = MyFirstNeuralNet()\n",
    "model.load_state_dict(torch.load(\"path/to/model_state_dict\"))\n",
    "```\n",
    "\n",
    "Alternatively, you can save/load the entire model using\n",
    "```\n",
    "torch.save(model, \"path/to/model\")\n",
    "\n",
    "model = torch.load(\"path/to/model\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "063b6ab4",
   "metadata": {},
   "source": [
    "### Task 3.2 - Model weights\n",
    "\n",
    "For this task, you will print out the trained model's `.state_dict()` and submit this to Coursemology.\n",
    "\n",
    "*Note: An acceptable loss value should be less than 1.0. If your loss is greater than 1, try re-running with a different random initialization, or adjust your model configuration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To submit this output\n",
    "print(\"--- Submit the OrderedDict below ---\")\n",
    "print(model.state_dict())\n",
    "\"\"\"\n",
    "state_dict = OrderedDict([]) # paste the output in\n",
    "\"\"\"\n",
    "\"\"\" YOUR CODE HERE \"\"\"\n",
    "raise NotImplementedError\n",
    "\"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5642530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model: nn.Module) -> int | float:\n",
    "    model.load_state_dict(state_dict)\n",
    "    x = torch.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "    y = torch.abs(x-1)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    y_pred = model.forward(x)\n",
    "    return loss_fn(y_pred, y).item()\n",
    "\n",
    "assert model.load_state_dict(state_dict)\n",
    "assert get_loss(model) < 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f3d9a8a",
   "metadata": {},
   "source": [
    "### Concept 3.2 - Using NN to recognize handwritten digits\n",
    "\n",
    "Now we will be building a neural network to classify images to their respective digits.  \n",
    "\n",
    "You will build and train a model on the classic **MNIST Handwritten Digits** dataset. Each grayscale image is a $28 \\times 28$ matrix/tensor that looks like so:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"500\" />\n",
    "\n",
    "MNIST is a classification problem and the task is to take in an input image and classify them into one of ten buckets: the digits from $0$ to $9$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "433b0e36",
   "metadata": {},
   "source": [
    "### Demo 3.5 - Loading an external dataset\n",
    "\n",
    "The cell below imports the MNIST dataset, which is already pre-split into train and test sets.  \n",
    "\n",
    "The download takes approximately 63MB of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2add2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL – THIS DOWNLOADS THE MNIST DATASET\n",
    "# RUN THIS CELL BEFORE YOU RUN THE REST OF THE CELLS BELOW\n",
    "from torchvision import datasets\n",
    "\n",
    "# This downloads the MNIST datasets ~63MB\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True)\n",
    "mnist_test  = datasets.MNIST(\"./\", train=False, download=True)\n",
    "\n",
    "x_train = mnist_train.data.reshape(-1, 784) / 255\n",
    "y_train = mnist_train.targets\n",
    "    \n",
    "x_test = mnist_test.data.reshape(-1, 784) / 255\n",
    "y_test = mnist_test.targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e06d36",
   "metadata": {},
   "source": [
    "### Task 3.3 - Define the model architecture and implement the forward pass\n",
    "Create a 3-layer network in the `__init__` method of the model `DigitNet`.  \n",
    "These layers are all `Linear` layers and should correspond to the following the architecture:\n",
    "\n",
    "![PyTorch](images/img_linear_nn.png)\n",
    "\n",
    "In our data, a given image $x$ has been flattened from a 28x28 image to a 784-length array.\n",
    "\n",
    "After initializing the layers, stitch them together in the `forward` method. Your network should look like so:\n",
    "\n",
    "$$x \\rightarrow \\text{Linear(512)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(128)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(10)} \\rightarrow \\text{Softmax} \\rightarrow \\hat{y}$$\n",
    "\n",
    "**Softmax Layer**: The final softmax activation is commonly used for classification tasks, as it will normalizes the results into a vector of values that follows a probability distribution whose total sums up to 1. The output values are between the range [0,1] which is nice because we are able to avoid binary classification and accommodate as many classes or dimensions in our neural network model.\n",
    "\n",
    "*Note: When using `torch.softmax(...)` on the final layer, ensure you are applying it on the correct dimension (as you did in NumPy via the `axis` argument in popular methods)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, input_dimensions: int, num_classes: int): # set the arguments you'd need\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        - DO NOT hardcode the input_dimensions, use the parameter in the function\n",
    "        - Your network should work for any input and output size \n",
    "        - Create the 3 layers (and a ReLU layer) using the torch.nn layers API\n",
    "        \"\"\"\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Input tensor (batch size is the entire dataset)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The output of the entire 3-layer model.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        YOUR CODE\n",
    "        \n",
    "        - Pass the inputs through the sequence of layers\n",
    "        - Run the final output through the Softmax function on the right dimension!\n",
    "        \"\"\"\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "model = DigitNet(784, 10)\n",
    "assert [layer.detach().numpy().shape for name, layer in model.named_parameters()] \\\n",
    "        == [(512, 784), (512,), (128, 512), (128,), (10, 128), (10,)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "921fec58",
   "metadata": {},
   "source": [
    "### Task 3.4 - Training Loop\n",
    "\n",
    "As demonstrated in Section 3.2, implement the function `train_model` that performs the following for every epoch/iteration:\n",
    "\n",
    "1. set the optimizer's gradients to zero\n",
    "2. forward pass\n",
    "3. calculate the loss\n",
    "4. backpropagate using the loss\n",
    "5. take an optimzer step to update weights\n",
    "\n",
    "This time, use the Adam optimiser to train the network.\n",
    "<br/>\n",
    "<br/>\n",
    "Use Cross-Entropy Loss, since we are performing a classification.\n",
    "<br/>\n",
    "_(PyTorch Softmax normalize logits while CrossEntropyLoss accepts unnormalized logits and CrossEntropyLoss already applies LogSoftmax, however, we will use Softmax here as we want to showcase how Softmax can convert the raw scores produced by the network into a probability distribution over the classes)._\n",
    "<br/>\n",
    "<br/>\n",
    "Train for 20 epochs.  \n",
    "\n",
    "*Note: refer to the command glossary to find out how to instantiate optimisers, losses, and more*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7da065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train: torch.Tensor, y_train: torch.Tensor, epochs: int = 20):\n",
    "    \"\"\"\n",
    "    Trains the model for 20 epochs/iterations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        x_train : A tensor of training features of shape (60000, 784)\n",
    "        y_train : A tensor of training labels of shape (60000, 1)\n",
    "        epochs  : Number of epochs, default of 20\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        The final model \n",
    "    \"\"\"\n",
    "    model = DigitNet(784, 10)\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters()) # use Adam\n",
    "    loss_fn = nn.CrossEntropyLoss()   # use CrossEntropyLoss\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "    return model\n",
    "                \n",
    "digit_model = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "x_train_new = torch.rand(5, 784, requires_grad=True)\n",
    "y_train_new = ones = torch.ones(5, dtype=torch.uint8)\n",
    "\n",
    "assert type(train_model(x_train_new, y_train_new)) == DigitNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c70083f",
   "metadata": {},
   "source": [
    "### Demo 3.6 - Explore your model\n",
    "\n",
    "Now that we have trained the model, let us run some predictions on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5199255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration: You can use this cell for exploring your trained model\n",
    "\n",
    "idx = 0 # try on some index\n",
    "\n",
    "scores = digit_model(x_test[idx:idx+1])\n",
    "_, predictions = torch.max(scores, 1)\n",
    "print(\"true label:\", y_test[idx].item())\n",
    "print(\"pred label:\", predictions[0].item())\n",
    "\n",
    "plt.imshow(x_test[idx].numpy().reshape(28, 28), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d87efe74",
   "metadata": {},
   "source": [
    "### Task 3.5 - Evaluate the model\n",
    "\n",
    "Now that we have trained the model, we should evaluate it using our test set.  \n",
    "We will be using the accuracy (whether or not the model predicted the correct label) to measure the model performance.  \n",
    "\n",
    "Since our model takes in a (n x 784) tensor and returns a (n x 10) tensor of probability scores for each of the 10 classes, we need to convert the probability scores into the actual predictions by taking the index of the maximum probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(scores: torch.Tensor, labels: torch.Tensor) -> int | float:\n",
    "    \"\"\"\n",
    "    Helper function that returns accuracy of model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        scores : The raw softmax scores of the network\n",
    "        labels : The ground truth labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Accuracy of the model. Return a number in range [0, 1].\n",
    "        0 means 0% accuracy while 1 means 100% accuracy\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\"\n",
    "scores = digit_model(x_test) # n x 10 tensor\n",
    "get_accuracy(scores, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "scores = torch.tensor([[0.4118, 0.6938, 0.9693, 0.6178, 0.3304, 0.5479, 0.4440, 0.7041, 0.5573,\n",
    "         0.6959],\n",
    "        [0.9849, 0.2924, 0.4823, 0.6150, 0.4967, 0.4521, 0.0575, 0.0687, 0.0501,\n",
    "         0.0108],\n",
    "        [0.0343, 0.1212, 0.0490, 0.0310, 0.7192, 0.8067, 0.8379, 0.7694, 0.6694,\n",
    "         0.7203],\n",
    "        [0.2235, 0.9502, 0.4655, 0.9314, 0.6533, 0.8914, 0.8988, 0.3955, 0.3546,\n",
    "         0.5752],\n",
    "        [0,0,0,0,0,0,0,0,0,1]])\n",
    "y_true = torch.tensor([5, 3, 6, 4, 9])\n",
    "acc_true = 0.4\n",
    "assert isclose(get_accuracy(scores, y_true),acc_true) , \"Mismatch detected\"\n",
    "print(\"passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21d43acd",
   "metadata": {},
   "source": [
    "# 4 Vision Layers\n",
    "Here, you'll be building the two fundamental layers that are the cornerstone of Computer Vision: the convolutional layer and the pooling layer (specifically, __max__ pooling). \n",
    "\n",
    "## Task 4.1: Convolution Under The Hood\n",
    "\n",
    "Your task is to write the `conv2d` function that performs the convolution operation on a 2-dim image, `img : torch.Tensor`, using a certain kernel, `kernel : torch.Tensor`. Assume there is no padding and the stride is 1. \n",
    "\n",
    "> Don't work on the channels – assume they remain the same. Your code should only work on the spatial dimensions: Height and Width.\n",
    "\n",
    "We've given you two images `x1` and `x2` and their convolutional outputs `c1` and `c2` respectively. Run them to verify whether your function is working as expected.\n",
    "\n",
    "$$\n",
    "c1 = \\texttt{conv2d}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "    4 & 9 & 3 & 0 & 3 \\\\\n",
    "    9 & 7 & 3 & 7 & 3 \\\\\n",
    "    1 & 6 & 6 & 9 & 8 \\\\\n",
    "    6 & 6 & 8 & 4 & 3 \\\\\n",
    "    6 & 9 & 1 & 4 & 4 \\\\\n",
    "\\end{bmatrix},~\n",
    "\\begin{bmatrix}\n",
    "    1 & 1 \\\\\n",
    "    1 & 1\n",
    "\\end{bmatrix}\\Bigg) = \n",
    "    \\begin{bmatrix} \n",
    "        4+9+9+7 & 9+3+7+3 & 3+0+3+7 & 0+3+7+3 \\\\\n",
    "        9+7+1+6 & 7+3+6+6 & 3+7+6+9 & 7+3+9+8  \\\\\n",
    "        1+6+6+6 & 6+6+6+8 & 6+9+8+4 & 9+8+4+3 \\\\\n",
    "        6+6+6+9 & 6+8+9+1 & 8+4+1+4 & 4+3+4+4 \\\\\n",
    "    \\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "        29 & 22 & 13 & 13 \\\\\n",
    "        23 & 22 & 25 & 27  \\\\\n",
    "        19 & 26 & 27 & 24 \\\\\n",
    "        27 & 24 & 17 & 15 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c2 = \\texttt{conv2d}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "    1 & 9 & 9 & 9 & 0 & 1 \\\\\n",
    "    2 & 3 & 0 & 5 & 5 & 2 \\\\\n",
    "    9 & 1 & 8 & 8 & 3 & 6 \\\\\n",
    "    9 & 1 & 7 & 3 & 5 & 2 \\\\\n",
    "    1 & 0 & 9 & 3 & 1 & 1 \\\\\n",
    "    0 & 3 & 6 & 6 & 7 & 9 \\\\\n",
    "\\end{bmatrix},~\n",
    "\\begin{bmatrix}\n",
    "    6 & 3 & 4 & 5 \\\\\n",
    "    0 & 8 & 2 & 8 \\\\\n",
    "    2 & 7 & 5 & 0 \\\\\n",
    "    0 & 8 & 1 & 9 \\\\\n",
    "\\end{bmatrix}\\Bigg) = \\begin{bmatrix} \n",
    "    285 & 369 & 286 \\\\\n",
    "    230 & 317 & 257 \\\\ \n",
    "    306 & 374 & 344 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "__Note:__ You are not allowed to use the `torch.nn.functional.conv2d` function. But nested loop is allowed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "def conv2d(img: torch.Tensor, kernel: torch.Tensor):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        img: the 2-dim image with a specific height and width\n",
    "        kernel: a 2-dim kernel (smaller than image dimensions) that convolves the given image\n",
    "    \n",
    "    RETURNS\n",
    "        the convolved 2-dim image\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "x1 = torch.tensor([\n",
    "    [4, 9, 3, 0, 3],\n",
    "    [9, 7, 3, 7, 3],\n",
    "    [1, 6, 6, 9, 8],\n",
    "    [6, 6, 8, 4, 3],\n",
    "    [6, 9, 1, 4, 4]\n",
    "])\n",
    "k1 = torch.ones((2, 2))\n",
    "o1 = torch.tensor([\n",
    "    [29., 22., 13., 13.],\n",
    "    [23., 22., 25., 27.],\n",
    "    [19., 26., 27., 24.],\n",
    "    [27., 24., 17., 15.]\n",
    "])\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [1, 9, 9, 9, 0, 1],\n",
    "    [2, 3, 0, 5, 5, 2],\n",
    "    [9, 1, 8, 8, 3, 6],\n",
    "    [9, 1, 7, 3, 5, 2],\n",
    "    [1, 0, 9, 3, 1, 1],\n",
    "    [0, 3, 6, 6, 7, 9]\n",
    "])\n",
    "k2 = torch.tensor([\n",
    "    [6, 3, 4, 5],\n",
    "    [0, 8, 2, 8],\n",
    "    [2, 7, 5, 0],\n",
    "    [0, 8, 1, 9]\n",
    "])\n",
    "o2 = torch.tensor([\n",
    "    [285., 369., 286.],\n",
    "    [230., 317., 257.],\n",
    "    [306., 374., 344.]\n",
    "])\n",
    "\n",
    "# TEST YOUR conv2d FUNCTION HERE\n",
    "c1 = conv2d(x1, k1)\n",
    "print(c1, torch.all(torch.eq(c1, o1)).item())\n",
    "c2 = conv2d(x2, k2)\n",
    "print(c2, torch.all(torch.eq(c2, o2)).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6b3f99f",
   "metadata": {},
   "source": [
    "## Task 4.2: Max Pooling Under The Hood\n",
    "\n",
    "Your task is to write the `maxpool2d` function that takes in an image, `img : torch.Tensor`, and a square kernel size `size : int`. Assume stride is 1 and there's no padding.\n",
    "\n",
    "We've given you two images `x1` and `x2` to test your `maxpool2d` function with `size=2` and `size=3` respectively.  \n",
    "$$\n",
    "m1 = \\texttt{maxpool2d}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "    4 & 9 & 3 & 0 & 3 \\\\\n",
    "    9 & 7 & 3 & 7 & 3 \\\\\n",
    "    1 & 6 & 6 & 9 & 8 \\\\\n",
    "    6 & 6 & 8 & 4 & 3 \\\\\n",
    "    6 & 9 & 1 & 4 & 4 \\\\\n",
    "\\end{bmatrix},~2\\Bigg) =\n",
    "\\begin{bmatrix} \n",
    "        max(4,9,9,7) & max(9,3,7,3) & max(3,0,3,7) & max(0,3,7,3) \\\\\n",
    "        max(9,7,1,6) & max(7,3,6,6) & max(3,7,6,9) & max(7,3,9,8)  \\\\\n",
    "        max(1,6,6,6) & max(6,6,6,8) & max(6,9,8,4) & max(9,8,4,3) \\\\\n",
    "        max(6,6,6,9) & max(6,8,9,1) & max(8,4,1,4) & max(4,3,4,4) \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "        9 & 9 & 7 & 7 \\\\\n",
    "        9 & 7 & 9 & 9  \\\\\n",
    "        6 & 8 & 9 & 9 \\\\\n",
    "        9 & 9 & 8 & 4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "m2 = \\texttt{maxpool2d}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "    1 & 9 & 9 & 9 & 0 & 1 \\\\\n",
    "    2 & 3 & 0 & 5 & 5 & 2 \\\\\n",
    "    9 & 1 & 8 & 8 & 3 & 6 \\\\\n",
    "    9 & 1 & 7 & 3 & 5 & 2 \\\\\n",
    "    1 & 0 & 9 & 3 & 1 & 1 \\\\\n",
    "    0 & 3 & 6 & 6 & 7 & 9 \\\\\n",
    "\\end{bmatrix},~3\\Bigg) = \\begin{bmatrix} \n",
    "    9 & 9 & 9 & 9 \\\\\n",
    "    9 & 8 & 8 & 8 \\\\ \n",
    "    9 & 9 & 9 & 8 \\\\\n",
    "    9 & 9 & 9 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "__Note:__ You are not allowed to use the `torch.nn.functional.max_pool2d` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "def maxpool2d(img: torch.Tensor, size: int):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "        img: the 2-dim image with a specific height and width\n",
    "        size: an integer corresponding to the window size for Max Pooling\n",
    "    \n",
    "    RETURNS\n",
    "        the 2-dim output after Max Pooling\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cebc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "x1 = torch.tensor([\n",
    "    [4, 9, 3, 0, 3],\n",
    "    [9, 7, 3, 7, 3],\n",
    "    [1, 6, 6, 9, 8],\n",
    "    [6, 6, 8, 4, 3],\n",
    "    [6, 9, 1, 4, 4]\n",
    "])\n",
    "k1 = 2\n",
    "o1 = torch.tensor([\n",
    "    [9., 9., 7., 7.],\n",
    "    [9., 7., 9., 9.],\n",
    "    [6., 8., 9., 9.],\n",
    "    [9., 9., 8., 4.]\n",
    "])\n",
    "\n",
    "x2 = torch.tensor([\n",
    "    [1, 9, 9, 9, 0, 1],\n",
    "    [2, 3, 0, 5, 5, 2],\n",
    "    [9, 1, 8, 8, 3, 6],\n",
    "    [9, 1, 7, 3, 5, 2],\n",
    "    [1, 0, 9, 3, 1, 1],\n",
    "    [0, 3, 6, 6, 7, 9]\n",
    "])\n",
    "k2 = 3\n",
    "o2 = torch.tensor([\n",
    "    [9., 9., 9., 9.],\n",
    "    [9., 8., 8., 8.],\n",
    "    [9., 9., 9., 8.],\n",
    "    [9., 9., 9., 9.]\n",
    "])\n",
    "\n",
    "# TEST YOUR maxpool2d FUNCTION HERE\n",
    "m1 = maxpool2d(x1, k1)\n",
    "print(m1, torch.all(torch.eq(m1, o1)).item())\n",
    "m2 = maxpool2d(x2, k2)\n",
    "print(m2, torch.all(torch.eq(m2, o2)).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45afe4d9",
   "metadata": {},
   "source": [
    "# 5: MNIST Classification with CNNs\n",
    "\n",
    "We will be working on the MNIST handwritten digits classification problem again but this time with CNNs. You'll be working the images as they are in the form of $1 \\times 28 \\times 28$ tensors, where $28$ is the image height and width, and $1$ is the number of colour channels (grayscale image in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not remove this cell\n",
    "# run this before moving on\n",
    "\n",
    "T = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "Note: If you updated the path to the directory containing `MNIST` \n",
    "directory, please update it here as well.\n",
    "\"\"\"\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=False, transform=T)\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=False, transform=T)\n",
    "\n",
    "\"\"\"\n",
    "if you feel your computer can't handle too much data, you can reduce the batch\n",
    "size to 64 or 32 accordingly, but it will make training slower. \n",
    "\n",
    "We recommend sticking to 128 but do choose an appropriate batch size that your\n",
    "computer can manage. The training phase tends to require quite a bit of memory.\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, shuffle=True, batch_size=256)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "698c6b40",
   "metadata": {},
   "source": [
    "## Concept 5.1: DataLoaders\n",
    "\n",
    "PyTorch __DataLoaders__ accept datasets and can iterate through the datasets as we deem fit.\n",
    "\n",
    "`train_loader = torch.utils.data.DataLoader(mnist_train, shuffle=True, batch_size=256)` means that this dataloader takes in the MNIST training data, and outputs training features and labels in batches of 256. It also reshuffles all the data in the dataset for the next epoch once it has outputted all the data in the dataset.\n",
    "\n",
    "Run the following code to get a better idea of how dataloaders work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to code\n",
    "# run this before moving on\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ffc08d2",
   "metadata": {},
   "source": [
    "## Task 5.1: Building a Vanilla ConvNet\n",
    "\n",
    "Your task here is to build a ConvNet using PyTorch layers. You can refer to the attached command glossary to read more about the layers. Use the following architecture:\n",
    "\n",
    "$$\n",
    "\\text{Conv(32, (3,3))} \\rightarrow \\text{MP(2,2)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{Conv(64, (3,3))} \\rightarrow \\text{MP(2,2)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{Flat} \\\\ \\rightarrow \\text{L(1600, 256)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{L(256, 128)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{L(128, 10)} \\rightarrow \\text{Softmax}\n",
    "$$\n",
    "\n",
    "where \n",
    "- [`Conv`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) is a Convolution layer with the specified output channels and kernel size, with no padding and a stride of 1 by default.\n",
    "\n",
    "- [`MP`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) is the Max Pooling layer with the specified kernel size, with no padding, the stride set to the same shape as the kernel by default.\n",
    "\n",
    "- [`LReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) is Leaky ReLU with the specified negative slope.\n",
    "\n",
    "- `Flat` is the flattening operation, which should flatten/reshape the tensor from a multi-dimensional tensor (batch_size, num_channels, width, height) into a \"flat\" tensor (batch_size, num_channels x width x height). The 2-dimensional result represents that each sample has only 1 dimension of \"flattened\" data. This has already been implemented for you\n",
    "\n",
    "- [`L`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a fully-connected layer with the specified input and output features.\n",
    "\n",
    "__Note:__ For all your networks hereon, the only constructor argument is `classes`. Do not add any other parameters to the `__init__` method. Remember not to hardcode for the number of classes and use the `classes` argument instead.\n",
    "\n",
    "__Note:__ There is no need to include a Softmax layer in your neural network, as technically, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which we are going to use as our loss function later, already applies Softmax implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model using Conv2d and MaxPool2d layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, classes: int):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        classes: integer that corresponds to the number of classes for MNIST\n",
    "        \"\"\"\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "        x = x.view(-1, 64*5*5) # Flattening – do not remove this line\n",
    "\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "\n",
    "# Test cases\n",
    "# Test your network's forward pass\n",
    "num_samples, num_channels, width, height = 20, 1, 28, 28\n",
    "x = torch.rand(num_samples, num_channels, width, height)\n",
    "net = RawCNN(10)\n",
    "y = net(x)\n",
    "print(y.shape) # torch.Size([20, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df3949cc",
   "metadata": {},
   "source": [
    "## Concept 5.2: Dropout\n",
    "\n",
    "__Dropout__ (*Srivastava et al., 2014*) is a regularisation technique that randomly shuts off neurons in a given layer. This means the output of the neuron is __zero__. As users, we need to specify a probability value `p` that is the probability of a neuron being shut off or not; there's a $p$ chance of a neuron being shut off.\n",
    "\n",
    "Suppose a layer has $n$ neurons/units. Mathematically, \n",
    "\n",
    "$$\n",
    "\\text{Prob}(i = 1) = p \\\\\n",
    "\\text{Prob}(i = 0) = 1 - p\n",
    "$$ \n",
    "\n",
    "where $i \\in \\{1, \\dots, n\\}$ and $1$ represents neuron $i$ being shut off and $0$ represents neuron $0$ left untouched.\n",
    "\n",
    "Essentially, Dropout does this:\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png\" width=600>\n",
    "\n",
    "### Why Dropout works\n",
    "By randomly dropping/zero-ing out neurons in a layer, it has a regularising effect on the model. It prevents overfitting because the loss of certain features means the model doesn't accidentally compute very complex functions to model the relationship between $x$ and $y$.\n",
    "\n",
    "#### Dropout in PyTorch\n",
    "To use Dropout in a network, we can create a `Dropout` layer in our `__init__` method of the model class:\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ..., drop_prob):\n",
    "        super().__init__()\n",
    "        self.l1 = ...\n",
    "        ...\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        ...\n",
    "        self.ln = ...\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        ...\n",
    "        x = self.dropout(x)\n",
    "        ...\n",
    "        out = ...\n",
    "        \n",
    "        return out\n",
    "```\n",
    "\n",
    "__Note:__ Other that `nn.Dropout`, there is a `nn.Dropout2d` in PyTorch. Instead of randomly zero-ing out neurons, `Dropout2d` randomly zero-es out the entire channels of the input. \n",
    "\n",
    "`nn.Dropout` is best used with non-spatial data or data that has been flattened, which is typical for fully connected layers. `nn.Dropout2d` is designed for spatial data, making it ideal for use right after convolutional and pooling layers in CNNs.\n",
    "\n",
    "For the sake of this problem set, You should choose one of them to be but __NOT both__ in your neural network.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a23c4c6",
   "metadata": {},
   "source": [
    "## Task 5.2: Building a ConvNet with Dropout\n",
    "\n",
    "Here, you must build the exact same network but with Dropout inside the architecture. You can refer to the attached command glossary to read more about the layers. Use the following architecture:\n",
    "\n",
    "$$\n",
    "\\text{Conv(32, (3,3))} \\rightarrow \\text{MP(2,2)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\textbf{DO(prob)} \\rightarrow \\\\\n",
    "\\text{Conv(64, (3,3))} \\rightarrow \\text{MP(2,2)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\textbf{DO(prob)} \\rightarrow \\\\\n",
    "\\text{Flat} \\rightarrow \\text{L(1600, 256)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\textbf{DO(prob)} \\rightarrow \\\\\n",
    "\\text{L(256, 128)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{L(128, 10)} \\rightarrow \\text{Softmax}\n",
    "$$\n",
    "\n",
    "where \n",
    "- [`Conv`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) is a Convolution layer with the specified output channels and kernel size, with no padding and a stride of 1 by default.\n",
    "\n",
    "- [`MP`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) is the Max Pooling layer with the specified kernel size, with no padding, the stride set to the same shape as the kernel by default.\n",
    "\n",
    "- [`LReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) is Leaky ReLU with the specified negative slope.\n",
    "\n",
    "- `Flat` is the flattening operation, which should flatten/reshape the tensor from a multi-dimensional tensor (batch_size, num_channels, width, height) into a \"flat\" tensor (batch_size, num_channels x width x height). The 2-dimensional result represents that each sample has only 1 dimension of \"flattened\" data. This has already been implemented for you\n",
    "\n",
    "- [`L`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a fully-connected layer with the specified input and output features.\n",
    " \n",
    "- [`DO`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) is Dropout with a dropping probability. Choose between `nn.Dropout` and `nn.Dropout2d` but __not both__ for your network.\n",
    "\n",
    "You are highly encouraged to initialise all your layers in the `__init__` method.\n",
    "\n",
    "__Reminder:__ Do not hardcode for the number of classes and the dropout probability. Use the `classes` and `drop_prob` constructor arguments instead.\n",
    "\n",
    "__Note:__ There is no need to include a Softmax layer in your neural network, as technically, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which we are going to use as our loss function later, already applies Softmax implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that uses Conv2d, MaxPool2d, and Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, classes: int, drop_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        classes: integer that corresponds to the number of classes for MNIST\n",
    "        drop_prob: probability of dropping a node in the neural network\n",
    "        \"\"\"\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "        x = x.view(-1, 64*5*5) # Flattening – do not remove\n",
    "\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "# Test your network's forward pass\n",
    "num_samples, num_channels, width, height = 20, 1, 28, 28\n",
    "x = torch.rand(num_samples, num_channels, width, height)\n",
    "net = DropoutCNN(10)\n",
    "y = net(x)\n",
    "print(y.shape) # torch.Size([20, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53c3b117",
   "metadata": {},
   "source": [
    "## Task 5.3: Training your Vanilla and Dropout CNNs\n",
    "\n",
    "Here, write down the training loop in the function `train_model` to train the CNNs you have just created. It will take in the respective NN (vanilla or dropout), as well as training and testing __data loaders__ (more on this later) that return batches of images and their respective labels to train on. \n",
    "\n",
    "Use the `torch.optim.Adam(...)` optimizer and Cross Entropy Loss.\n",
    "\n",
    "> Return the model and epoch losses.\n",
    "\n",
    "Remember to extract the loss value from the `loss` tensor by using `loss.item()`.\n",
    "\n",
    "__Tip:__ Don't be worried if your model takes a while to train. Your mileage may also vary depending on your CPU. But if you would like to speed things up, you can consider making use of your device's GPU to parallelize the matrix computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# do not remove the above line\n",
    "def train_model(loader: torch.utils.data.DataLoader, model: nn.Module):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    loader: the data loader used to generate training batches\n",
    "    model: the model to train\n",
    "  \n",
    "    RETURNS\n",
    "        the final trained model and losses\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \n",
    "    - create the loss and optimizer\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\"\n",
    "    epoch_losses = []\n",
    "    for i in range(10):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for idx, data in enumerate(loader):\n",
    "            x, y = data\n",
    "            \"\"\"\n",
    "            YOUR CODE HERE\n",
    "            \n",
    "            - reset the optimizer\n",
    "            - perform forward pass\n",
    "            - compute loss\n",
    "            - perform backward pass\n",
    "            \"\"\"\n",
    "            \"\"\" YOUR CODE HERE \"\"\"\n",
    "            raise NotImplementedError\n",
    "            \"\"\" YOUR CODE END HERE \"\"\"\n",
    "\n",
    "        epoch_loss = epoch_loss / len(loader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "        \n",
    "\n",
    "    return model, epoch_losses\n",
    "\n",
    "print(\"======Training Vanilla Model======\")\n",
    "vanilla_model, losses = train_model(train_loader, RawCNN(10))\n",
    "print(\"======Training Dropout Model======\")\n",
    "do_model, losses = train_model(train_loader, DropoutCNN(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ecd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not remove – nothing to code here\n",
    "# run this cell before moving on\n",
    "# but ensure get_accuracy from task 3.5 is defined\n",
    "\n",
    "with torch.no_grad():\n",
    "    vanilla_model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_vanilla = vanilla_model(x)\n",
    "        acc = get_accuracy(pred_vanilla, y)\n",
    "        print(f\"vanilla acc: {acc}\")\n",
    "        \n",
    "    do_model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_do = do_model(x)\n",
    "        acc = get_accuracy(pred_do, y)\n",
    "        print(f\"drop-out (0.5) acc: {acc}\")\n",
    "        \n",
    "\"\"\"\n",
    "The network with Dropout might under- or outperform the network without\n",
    "Dropout. However, in terms of generalisation, we are assured that the Dropout\n",
    "network will not overfit – that's the guarantee of Dropout.\n",
    "\n",
    "A very nifty trick indeed!\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ddfda3",
   "metadata": {},
   "source": [
    "## Task 5.4: Observing Effects of Dropout\n",
    "\n",
    "Here, train your `DropoutCNN` with your `train_model(loader, model)` from Task 5.3, but with `p=0.1` and `p=0.95` respectively. \n",
    "\n",
    "Explain why extreme values of Dropout don't work as well on neural networks. Look back at first principles – what does Dropout do in the first place? How does the `p` value affect how it does it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1328f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# do not remove – nothing to code here\n",
    "# run this before moving on\n",
    "\n",
    "print(\"======Training Dropout Model with Dropout Probability 0.10======\")\n",
    "do10_model, do10_losses = train_model(train_loader, DropoutCNN(10, 0.10))\n",
    "print(\"======Training Dropout Model with Dropout Probability 0.95======\")\n",
    "do95_model, do95_losses = train_model(train_loader, DropoutCNN(10, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not remove – nothing to code here\n",
    "# run this cell before moving on\n",
    "# but ensure get_accuracy from task 3.5 is defined\n",
    "\n",
    "with torch.no_grad():\n",
    "    do10_model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_do = do10_model(x)\n",
    "        acc = get_accuracy(pred_do, y)\n",
    "        print(acc)\n",
    "\n",
    "    do95_model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred_do = do95_model(x)\n",
    "        acc = get_accuracy(pred_do, y)\n",
    "        print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e9eaf5",
   "metadata": {},
   "source": [
    "# Chapter 6: Training on CIFAR-10\n",
    "\n",
    "## Concept 6.1: CIFAR-10\n",
    "Using what you've learned with MNIST, apply the techniques to CIFAR-10, a dataset of 60K training and 10K testing images comprising of real-life objects corresponding to the following 10 classes:\n",
    "\n",
    "- airplane\t\t\t\t\t\t\t\t\t\t\n",
    "- automobile\t\t\t\t\t\t\t\t\t\t\n",
    "- bird\t\t\t\t\t\t\t\t\t\t\n",
    "- cat\t\t\t\t\t\t\t\t\t\t\n",
    "- deer\t\t\t\t\t\t\t\t\t\t\n",
    "- dog\t\t\t\t\t\t\t\t\t\t\n",
    "- frog\t\t\t\t\t\t\t\t\t\t\n",
    "- horse\t\t\t\t\t\t\t\t\t\t\n",
    "- ship\t\t\t\t\t\t\t\t\t\t\n",
    "- truck\n",
    "\n",
    "Each image is $3 \\times 32 \\times 32$, meaning we operate on 3 color channels RGB. Some of these images look like so:\n",
    "\n",
    "![PyTorch](images/cifar.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c0aab70",
   "metadata": {},
   "source": [
    "## Concept 6.2: Data Augmentation\n",
    "\n",
    "In reality, however, finding a well-representative, balanced dataset is difficult. To address this issue, we use __Data Augmentation__. It refers to the process of transforming data in a training dataset in one or more ways to create more samples to expand the training dataset.\n",
    "\n",
    "Here, we will pick images from the original dataset `x_train`, perform some transformations $F$ on them, and append them to `x_train`. So, for example, if I have a training dataset of 200 car images, I can perform augmentations on the 200 images to get 300 more images, thereby making my new training dataset 500 images large.\n",
    "\n",
    "Of course, the impact of data augmentation on model training depends on the types of augmentation used. Here are some common ones Computer Vision practitioners use:\n",
    "\n",
    "- Normalisation\n",
    "- Horizontal and Vertical Flipping\n",
    "- Rotation\n",
    "- Blurring\n",
    "- Adding noise\n",
    "- Skewing\n",
    "- Cropping (zooming in or out)\n",
    "- Brightness and Contrast\n",
    "- Shuffling pixels\n",
    "\n",
    "This results in a wide variety of new samples being created that can be used for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d593d799",
   "metadata": {},
   "source": [
    "## The `transforms` module\n",
    "\n",
    "Here, we are going to use the `transforms` module from PyTorch to transform the images in our dataset. It contains all kinds of image transformations from `rotate` to `resize`. Check out the full list of augmentations on the PyTorch documentation: https://pytorch.org/vision/stable/transforms.html.\n",
    "\n",
    "Explore the following example to see how the transformations work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train = datasets.CIFAR10(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(cifar_train_loader))\n",
    "img = train_features[0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip()\n",
    "                                # YOUR CODE HERE\n",
    "                                ]) # add in your own transformations to test\n",
    "tensor_img = transform(img)\n",
    "ax1.imshow(img.permute(1,2,0))\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title(\"Before Transformation\")\n",
    "ax2.imshow(tensor_img.permute(1, 2, 0))\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title(\"After Transformation\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57de4397",
   "metadata": {},
   "source": [
    "## Task 6.1: Picking Data Augmentations\n",
    "\n",
    "Your task is to pick your favourite data augmentations and apply them to the images from the dataset (in the later cell). \n",
    "\n",
    "We've already started you off with the necessary one `ToTensor()` that converts the original JPEG-format image to the PyTorch `Tensor` format. Refer to the command glossary to add your custom data augmentations from the list we've provided. \n",
    "\n",
    "**Choose at least 2 additional augmentations.** Tell us which augmentations you chose to use _and_ why. Then tell us which augmentations you avoided _and_ why. \n",
    "\n",
    "__Note:__ Feel free to use any augmentations you wish from the full list of augmentations shown on the [PyTorch documentation](https://pytorch.org/vision/stable/transforms.html)! There's no need to be restricted to the list that we've provided.\n",
    "\n",
    "The point is to improve your model performance as much as possible! Use trial and error to get the best performing network in Task 3.2!\n",
    "\n",
    "Be creative :D\n",
    "\n",
    "__Note:__ Do ensure your augmentations retain the 3-dimensional shape of the CIFAR-10 images. The final images should still have the shape `(3, 32, 32)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick your data augmentations here\n",
    "def get_augmentations() -> transforms.Compose:\n",
    "    T = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # YOUR CODE HERE\n",
    "    ])\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9440fb9",
   "metadata": {},
   "source": [
    "Create your data loaders that return batches of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not remove this cell\n",
    "# run this before moving on\n",
    "\n",
    "T = get_augmentations()\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"./\", train=True, download=True, transform=T)\n",
    "cifar_test = datasets.CIFAR10(\"./\", train=False, download=True, transform=T)\n",
    "\n",
    "\"\"\"\n",
    "if you feel your computer can't handle too much data, you can reduce the batch\n",
    "size to 64 or 32 accordingly, but it will make training slower. \n",
    "\n",
    "We recommend sticking to 128 but dochoose an appropriate batch size that your\n",
    "computer can manage. The training phase tends to require quite a bit of memory.\n",
    "\n",
    "CIFAR-10 images have dimensions 3x32x32, while MNIST is 1x28x28\n",
    "\"\"\"\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41135908",
   "metadata": {},
   "source": [
    "## Concept 6.3: Sequential Model Building with PyTorch\n",
    "\n",
    "All this while, you've been adding layers one by one as attributes inside the `__init__` method. This is so that you can quickly debug which layer(s) is causing issues later down the road. However, for the most part, there should be no major issues when creating parts of your network or your entire network. \n",
    "\n",
    "This is why PyTorch lets you combine layers together using the `nn.Sequential` API. It allows you to stack layers inside and chain layers together. It allows you to build isolated modules that can exist on their own (either within a `nn.Module` class or otherwise) and be used as independent \"mini models\" on data tensors. Refer to https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html for more information about combining PyTorch modules to create your own.\n",
    "\n",
    "__Note:__ You should not add an array of layers inside `nn.Sequential` i.e., it's `nn.Sequential(xyz, abc, mno)`, **not** `nn.Sequential([xyz, abc, mno])`.\n",
    "\n",
    "#### Demo 6.1: 3-layer Multilayer Perceptron for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = nn.Sequential(\n",
    "                nn.Linear(784, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 10),\n",
    "                nn.Softmax(1) # softmax dimension\n",
    "            )\n",
    "\n",
    "x = torch.rand(15, 784) # a batch of 15 MNIST images\n",
    "y = densenet(x) # here we simply run the sequential densenet on the `x` tensor\n",
    "print(y.shape) # a batch of 15 predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22d9a1a6",
   "metadata": {},
   "source": [
    "#### Demo 6.2: 2-layer ConvNet for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, (3,3)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (3,3)),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(36864, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 10),\n",
    "                nn.Softmax(1) # softmax dimension\n",
    "            )\n",
    "\n",
    "x = torch.rand(15, 1, 28, 28) # a batch of 15 MNIST images\n",
    "y = convnet(x) # here we simply run the sequential convnet on the `x` tensor\n",
    "print (y.shape) # a batch of 15 predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3feb2b1a",
   "metadata": {},
   "source": [
    "## Task 6.2: Build a ConvNet for CIFAR-10\n",
    "\n",
    "Your task is to build a decently-sized ConvNet (i.e., $\\geq 4$ layers). Design your ConvNet with the following architecture\n",
    "\n",
    "$$\n",
    "\\text{Conv(32, (3,3))} \\rightarrow \\text{MP((2,2))} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{Conv(64, (3,3))} \\rightarrow \\text{MP((2,2))} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{GAP} \\\\ \\rightarrow \\text{L(64, 256)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{L(256, 128)} \\rightarrow \\text{LReLU(0.1)} \\rightarrow \\text{L(128, 10)}\n",
    "$$\n",
    "\n",
    "where \n",
    "- [`Conv`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) is a Convolution layer with the specified output channels and kernel size\n",
    "\n",
    "- [`MP`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) is the Max Pooling layer with the specified kernel size\n",
    "\n",
    "- [`LReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) is Leaky ReLU with the specified negative slope\n",
    "\n",
    "- `GAP` is the Global Average Pooling operation (already implemented for you)\n",
    "\n",
    "- [`L`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a fully-connected layer with the specified input and output features\n",
    "\n",
    "---\n",
    "\n",
    "You must use the [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) API to build two parts:\n",
    "1. The `self.conv` attribute must contain all the Convolutional, Pooling, and Activation layers\n",
    "2. The `self.fc` attribute must contain all the fully-connected layers after the flattening\n",
    "\n",
    "The `self.conv` and `self.fc` attributes are already given to you. All you need to do is chain the arbitrary `nn.XYZ` layers together based on the architecture stated above.\n",
    "\n",
    "__Note:__ The flattening is already done for you via Global Average Pooling (GAP) in the `forward` method. Do not add the Softmax activation in the `self.fc` Sequential module.\n",
    "\n",
    "__Reminder:__ Do not hardcode for the number of classes. Use the `classes` argument instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARCNN(nn.Module):\n",
    "    def __init__(self, classes: int):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        classes: integer that corresponds to the number of classes for CIFAR-10\n",
    "        \"\"\"\n",
    "        self.conv = nn.Sequential(\n",
    "                        # YOUR CODE HERE\n",
    "                    )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "                        # YOUR CODE HERE\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        x = x.view(x.shape[0], 64, 6*6).mean(2) # GAP – do not remove this line\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd7fc3a",
   "metadata": {},
   "source": [
    "## Train your ConvNet on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do not remove – nothing to code here\n",
    "# run this cell before moving on\n",
    "\n",
    "cifar10_model, losses = train_model(cifar_train_loader, CIFARCNN(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6e68deb",
   "metadata": {},
   "source": [
    "## Test the CIFAR-10 ConvNet model using the testing data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not remove – nothing to code here\n",
    "# run this cell before moving on\n",
    "# but ensure get_accuracy from task 3.5 is defined\n",
    "\n",
    "with torch.no_grad():\n",
    "    cifar10_model.eval()\n",
    "    for i, data in enumerate(cifar_test_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = cifar10_model(x)\n",
    "        acc = get_accuracy(pred, y)\n",
    "        print(f\"cifar accuracy: {acc}\")\n",
    "        \n",
    "# don't worry if the CIFAR-10 accuracy is low, it's a tough dataset to crack.\n",
    "# as long as you get something shy of 50%, you should be alright!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "487b86e8",
   "metadata": {},
   "source": [
    "# Neural Networks on Sequential Data\n",
    "We will learn the details of RNN, as well as their applications to sequential data. In the next task, we will use an RNN to model and predict patterns in time-series data, specifically using sine wave.\n",
    "\n",
    "## Packages\n",
    "The necessary packages are imported in the cell below. Please make sure that you run this cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(2109)\n",
    "np.random.seed(2109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e349a8",
   "metadata": {},
   "source": [
    "# Chapter 7: Simple RNN to Learn Sine Wave\n",
    "\n",
    "In this task, you will create a simple RNN model to predict values of a sine wave based on prior inputs. The goal is for the RNN to learn the underlying pattern of the sine wave and use this understanding to predict unseen values in the sequence.\n",
    "\n",
    "Through this exercise, you'll develop an understanding of how RNNs handle dependencies over time and how they can be trained to make predictions on continuous data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86a6d376",
   "metadata": {},
   "source": [
    "## Task 7.1: RNN Cell\n",
    "\n",
    "<center><img src=\"images/rnn.png\" style=\"width:500px;height:300px\"></center>\n",
    "\n",
    "At the core of the RNN is the RNN cell, which processes one time step of the sequence at a time. For each time step, the cell updates its hidden state based on the current input and the previous hidden state.\n",
    "\n",
    "We first implement the computations for a single time step. The diagram below describes the operations for a single time step of an RNN cell.\n",
    "<center><img src=\"images/rnn_cell.png\" style=\"width:700px;height:300px;\"></center>\n",
    "\n",
    "__Note__: an RNN cell outputs the hidden state $h_t$, but the function that you'll implement `rnn_cell_forward`, also calculates the prediction $\\hat{y}_t$. Keep in mind that the activation functions within the RNN cell can be replaced with other activation functions. For this task, tanh and softmax are used, but they can be replaced by other activation functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, h_prev, Wxh, Whh, Why, bh, by):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell\n",
    "\n",
    "    Args:\n",
    "        xt: 2D tensor of shape (nx, m)\n",
    "            Input data at timestep \"t\"\n",
    "        h_prev: 2D tensor of shape (nh, m)\n",
    "            Hidden state at timestep \"t-1\"\n",
    "        Wxh: 2D tensor of shape (nx, nh)\n",
    "            Weight matrix multiplying the input\n",
    "        Whh: 2D tensor of shape (nh, nh)\n",
    "            Weight matrix multiplying the hidden state\n",
    "        Why: 2D tensor of shape (nh, ny)\n",
    "            Weight matrix relating the hidden-state to the output\n",
    "        bh: 1D tensor of shape (nh, 1)\n",
    "            Bias relating to next hidden-state\n",
    "        by: 2D tensor of shape (ny, 1)\n",
    "            Bias relating the hidden-state to the output\n",
    "\n",
    "    Returns:\n",
    "        yt_pred -- prediction at timestep \"t\", tensor of shape (ny, m)\n",
    "        h_next -- next hidden state, of shape (nh, m)\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "public_paras = {\n",
    "    'xt': torch.tensor([[1., 1., 2.], [2., 1., 3.], [3., 5., 3.]]),\n",
    "    'h_prev': torch.tensor([[5., 3., 2.], [1., 3., 2.]]),\n",
    "    'Wxh': torch.tensor([[2., 2.], [3., 4.], [4., 3.]]),\n",
    "    'Whh': torch.tensor([[2., 4.], [2., 3.]]),\n",
    "    'Why': torch.tensor([[3., 5.], [5., 4.]]),\n",
    "    'bh': torch.tensor([[1.], [2.]]),\n",
    "    'by': torch.tensor([[3.], [1.]]),\n",
    "}\n",
    "\n",
    "expected_yt_pred = torch.tensor([[0.7311, 0.7311, 0.7311], [0.2689, 0.2689, 0.2689]])\n",
    "expected_h_next = torch.tensor([[1., 1., 1.], [1., 1., 1.]])\n",
    "\n",
    "actual_yt_pred, actual_h_next = rnn_cell_forward(**public_paras)\n",
    "assert torch.allclose(actual_yt_pred, expected_yt_pred, atol=1e-4)\n",
    "assert torch.allclose(actual_h_next, expected_h_next, atol=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16aaf900",
   "metadata": {},
   "source": [
    "## Task 7.2: Generate Sine Wave Data\n",
    "\n",
    "Use torch.linspace(start, end, steps) to create a sequence of evenly spaced values between 0 and $8\\pi$ over `num_time_steps` intervals. This will serve as the x-values (time points) for the sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a97e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_wave(num_time_steps):\n",
    "    \"\"\"\n",
    "    Generates a sine wave data\n",
    "\n",
    "    Args:\n",
    "        num_time_steps: int\n",
    "            Number of time steps\n",
    "    Returns:\n",
    "        data: 1D tensor of shape (num_time_steps,)\n",
    "            Sine wave data with corresponding time steps\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "num_time_steps_public = 5\n",
    "expected_data_public = torch.tensor([0.0000e+00, 1.7485e-07, 3.4969e-07, 4.7700e-08, 6.9938e-07])\n",
    "actual_data = generate_sine_wave(num_time_steps_public)\n",
    "\n",
    "assert torch.allclose(actual_data, expected_data_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 500\n",
    "sine_wave_data = generate_sine_wave(num_time_steps)\n",
    "\n",
    "# Plot the sine wave\n",
    "plt.plot(sine_wave_data)\n",
    "plt.title('Sine Wave')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aafe13e",
   "metadata": {},
   "source": [
    "## Task 7.3: Create sequences\n",
    "\n",
    "When training RNN, it's common to divide time series data into overlapping windows. The label used for comparison is the next value in the sequence.\n",
    "\n",
    "For example if we have series of $n$ data points and a window size of 3, the input sequences are $[x_1, x_2, x_3]$ to predict $x_4$, $[x_2, x_3, x_4]$ to predict $x_5$, $[x_3, x_4, x_5]$, to predict $x_6$, and so on.\n",
    "\n",
    "You will implement function `create_sequences` which generates sequences and their corresponding labels from a given sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(sine_wave, seq_length):\n",
    "    \"\"\"\n",
    "    Create overlapping sequences from the input time series and generate labels \n",
    "    Each label is the value immediately following the corresponding sequence.\n",
    "    \n",
    "    Args:\n",
    "        sine_wave: A 1D tensor representing the time series data (e.g., sine wave).\n",
    "        seq_length: int. The length of each sequence (window) to be used as input to the RNN.\n",
    "\n",
    "    Returns: \n",
    "        windows: 2D tensor where each row is a sequence (window) of length `seq_length`.\n",
    "        labels: 1D tensor where each element is the next value following each window.\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "seq_length_test = 2\n",
    "sine_wave_test = torch.tensor([0., 1., 2., 3.])\n",
    "expected_sequences = torch.tensor([[0., 1.], [1., 2.]])\n",
    "expected_labels = torch.tensor([2., 3.])\n",
    "\n",
    "actual_sequences, actual_labels = create_sequences(sine_wave_test, seq_length_test)\n",
    "assert torch.allclose(actual_sequences, expected_sequences)\n",
    "assert torch.allclose(actual_labels, expected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and labels\n",
    "seq_length = 20\n",
    "sequences, labels = create_sequences(sine_wave_data, seq_length)\n",
    "# Add extra dimension to match RNN input shape [batch_size, seq_length, num_features]\n",
    "sequences = sequences.unsqueeze(-1)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed35f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sequences into training data (first 80%) and test data (remaining 20%) \n",
    "train_size = int(len(sequences) * 0.8)\n",
    "train_seqs, train_labels = sequences[:train_size], labels[:train_size]\n",
    "test_seqs, test_labels = sequences[train_size:], labels[train_size:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ad1b640",
   "metadata": {},
   "source": [
    "## Task 7.4: Building RNN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23790704",
   "metadata": {},
   "source": [
    "Model Architecture\n",
    "$$x \\rightarrow \\text{RNN} \\rightarrow \\text{Linear}(1)$$\n",
    "\n",
    "- [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html): a basic recurrent layer in PyTorch. It takes an input of sequences and returns output for each time step, and the hidden state of the last time step. The `hidden_size` determines the number of hidden units in the RNN.\n",
    " \n",
    "- [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): fully connected layer. The output from the RNN's last hidden state will be passed through this layer to predict a single value (the next time step in the series)\n",
    "\n",
    "__Note:__ For all your networks hereon, the only constructor argument is `classes`. Do not add any other parameters to the `__init__` method. Remember not to hardcode and use the `classes` argument instead. For RNN layer, use `batch_first=True` to ensure that the batch dimension is handled as the first input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the SineRNN model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of input features per time step (typically 1 for univariate time series).\n",
    "            hidden_size (int): The number of units in the RNN's hidden layer.\n",
    "            output_size (int): The size of the output (usually 1 for predicting a single value).\n",
    "        \"\"\"\n",
    "        super(SineRNN, self).__init__()\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25110f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "input_size = output_size = 1\n",
    "hidden_size = 50\n",
    "model = SineRNN(input_size, hidden_size, output_size).to(device)\n",
    "assert [layer.detach().numpy().shape for _, layer in model.named_parameters()]\\\n",
    "      == [(50, 1), (50, 50), (50,), (50,), (1, 50), (1,)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1775af81",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function, and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(train_seqs)\n",
    "    loss = criterion(outputs.squeeze(), train_labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27a67379",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "\n",
    "Once training is complete, we can evaluate the model by plotting its predictions against the actual sine wave. We use a portion of test data that the model hasn't seen during training to check how well it generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on unseen data\n",
    "model.eval()\n",
    "y_pred = []\n",
    "input_seq = test_seqs[0]  # Start with the first testing sequence\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(len(test_seqs)):\n",
    "        output = model(input_seq)\n",
    "        y_pred.append(output.item())\n",
    "        \n",
    "        # Use the predicted value as the next input sequence\n",
    "        next_seq = torch.cat((input_seq[1:, :], output.unsqueeze(0)), dim=0)\n",
    "        input_seq = next_seq\n",
    "\n",
    "# Plot the true sine wave and predictions\n",
    "plt.plot(sine_wave_data, c='gray', label='Actual data')\n",
    "plt.scatter(np.arange(seq_length + len(train_labels)), sine_wave_data[:seq_length + len(train_labels)], marker='.', label='Train')\n",
    "x_axis_pred = np.arange(len(sine_wave_data) - len(test_labels), len(sine_wave_data))\n",
    "plt.scatter(x_axis_pred, y_pred, marker='.', label='Predicted')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b585b00",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right snippets of code into the corresponding box that says \"Your answer,\"and click \"Save.\" After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click \"Finalize submission.\" Note that once your submission is finalized, it is considered to be submitted for grading and cannot be changed. If you need to undo this action, you will have to email your assigned tutor for help. Please do not finalize your submission until you are sure that you want to submit your solutions for grading.\n",
    "\n",
    "*Have fun and enjoy coding.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
