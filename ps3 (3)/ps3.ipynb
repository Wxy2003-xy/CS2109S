{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 Part A: Predicting House Prices in Singapore\n",
    "\n",
    "**Release Date:** 7 October 2024 1800H\n",
    "\n",
    "**Due Date:** 19 October 2024 2359H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We have learned how to solve a regression problem using linear regression in class.\n",
    "In Part A of this problem set, we will apply our knowledge to solve a real-world problem. More\n",
    "specifically, we will develop linear regression and polynomial regression models to predict\n",
    "house prices in Singapore.\n",
    "\n",
    "**Required Files**:\n",
    "* ps3.ipynb\n",
    "* housing_data.csv\n",
    "\n",
    "**Plagiarism Policy**: Please refer to our [Course Policies](https://canvas.nus.edu.sg/courses/62323/pages/course-policies).\n",
    "\n",
    "**IMPORTANT**: While it is possible to write and run Python code directly in Jupyter notebook, we recommend that you do this Problem set with an IDE using the .py file provided. An IDE will make debugging significantly easier.\n",
    "\n",
    "**Post-Problem Set Survey**:\n",
    "Your feedback is important to us! After completing Problem Set 3, please take a moment to share your thoughts by filling out this [survey](https://coursemology.org/courses/2851/surveys/2410)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation to files\n",
    "\n",
    "**ps3.ipynb**:\n",
    "The template for all your tasks is provided in this file. Some test cases have\n",
    "been provided for you to check the output of your algorithm against the expected result. The tests are **not** comprehensive, and you are\n",
    "encouraged to write your own tests to check for correctness.\n",
    "\n",
    "**housing.csv**:\n",
    "There are 90 housing data points. Each data point consists of 3 features:\n",
    "* **floor_area_sqm** - size of the house in square meters\n",
    "* **bedrooms** - number of bedrooms\n",
    "* **schools** - number of primary schools within a 1km radius\n",
    "\n",
    "Our target value is the **asking_price**, which is the price of the housing unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "\n",
    "Similar to PS0, your implementation in the following tasks **should not\n",
    "involve any iteration, including `map` and `filter`, or recursion**. Instead, please work\n",
    "with the operations available in NumPy<sup>&#x2020;</sup>. Solutions that violate this will be penalised.\n",
    "\n",
    "There is, however, an exception for **Tasks 2.4 and 3.4**. In the pseudo-code for the\n",
    "algorithm required, there is an explicit for loop for task 2.4 and a while loop for task 3.4. Hence, **only for these tasks**, you\n",
    "may use a **single for/while loop** to iterate for the number of epochs required.\n",
    "\n",
    "<sup>&#x2020;</sup> You are allowed to use any mathematical functions, but this **does not mean that you are allowed to\n",
    "use *any* NumPy function** (there are NumPy functions that aren’t mathematical functions). For example,\n",
    "`np.vectorize` is not allowed since it is iterative. If you are in doubt about which functions are allowed, please\n",
    "ask in the forum (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital imports and setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "###################\n",
    "# Helper function #\n",
    "###################\n",
    "def load_data(filepath):\n",
    "    '''\n",
    "    Load in the given csv filepath as a numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath (string) : path to csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X, y (np.ndarray, np.ndarray) : (m, num_features), (m,) numpy matrices\n",
    "    '''\n",
    "    *X, y = np.genfromtxt(\n",
    "        filepath,\n",
    "        delimiter=',',\n",
    "        skip_header=True,\n",
    "        unpack=True,\n",
    "    ) # default dtype: float\n",
    "    X = np.array(X, dtype=float).T # cast features to int type\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "data_filepath = 'housing_data.csv'\n",
    "X, y = load_data(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining cost functions\n",
    "\n",
    "We need to define cost functions before creating a linear regression model to calculate\n",
    "the error between our prediction and the true values. We will define two cost functions:\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Mean Squared Error (MSE)\n",
    "\n",
    "Write the function `mean_squared_error(y_true, y_pred)` that returns a number representing the mean squared error of the predictions.\n",
    "\n",
    "The formula for Mean Squared Error is as follows:\n",
    "$$ MSE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the vector with actual values, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $m$ is the number of samples in the\n",
    "training data.\n",
    "\n",
    "**Remark**: The formula here follows the lecture slides for consistency. In definitions and implementations elsewhere, the denominator is usually just $m$ instead of $2m$.\n",
    "\n",
    "**Hint**: Consider using `np.square` or `np.power`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate mean squared error between y_pred and y_true.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true (np.ndarray) : (N, 1) numpy matrix consists of true values\n",
    "    y_pred (np.ndarray)   : (N, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean squared error value.\n",
    "    '''\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    mse = np.mean(np.square(y_true - y_pred))\n",
    "    return mse\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "y_true, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_squared_error(y_true, y_pred) in [45.25, 90.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Mean Absolute Error (MAE)\n",
    "\n",
    "Write the function `mean_absolute_error(y_true, y_pred)` that returns a number representing the mean absolute error of the predictions.\n",
    "\n",
    "The formula for Mean Absolute Error is as follows:\n",
    "$$ MSE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{m} \\sum_{i=1}^{m}|\\hat{y}_i - y_i| $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the vector with actual values, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $m$ is the number of samples in the\n",
    "training data.\n",
    "\n",
    "**Hint**: Consider using `np.abs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate mean absolute error between y_pred and y_true.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true (np.ndarray) : (m, 1) numpy matrix consists of true values\n",
    "    y_pred (np.ndarray)   : (m, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean absolute error value.\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test cases\u001b[39;00m\n\u001b[1;32m      2\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m5\u001b[39m]]), np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m12\u001b[39m], [\u001b[38;5;241m15\u001b[39m]])\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9.5\u001b[39m\n",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCalculate mean absolute error between y_pred and y_true.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    The mean absolute error value.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" YOUR CODE HERE \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" YOUR CODE END HERE \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "y_true, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_absolute_error(y_true, y_pred) == 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression\n",
    "\n",
    "Now we’re ready to create our own linear regression model. We will try to approximate a linear function, which can be written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n $$\n",
    "\n",
    "where $y$ is the target value, $x_1, x_2, \\dots, x_n$ are feature values, and $w_0, w_1, \\dots, w_n$ are parameters. $w_0$ is meant to represent the bias term, while $w_1, \\dots, w_n$ are the feature weights.\n",
    "\n",
    "**Bias term**\n",
    "\n",
    "The bias term ($w_0$) is useful in capturing an inherent offset of the target values from the origin, i.e. they have some non-zero \"default\" or \"starting\" value. The bias term accounts for this default value in our model. Without a bias term (or bias = 0), our regression lines will pass through the origin, which might not be appropriate for the data in question.\n",
    "\n",
    "Consider the scatter plot below. The blue line is the best fitting line without a bias term, while the red line includes a non-zero bias. Since the blue line starts at the origin, it is unable to capture the offset of the points. In contrast, the red line starts higher (at around 5), and hence is better able to approximate the data.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/bias_scatter.png\" alt=\"bias vs no bias\" width=\"40%\">\n",
    "<figcaption style=\"text-align:center\">Figure 0: Example of models with bias vs without bias.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Usually, we have to explicitly add a bias term into our data when building our models. In the following tasks, you'll explore how to do so and how this choice can affect the accuracy of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Adding a bias column\n",
    "\n",
    "In the lecture, we learned that adding a bias column allows our linear model to be more\n",
    "flexible. Write the function `add_bias_column(X)` that takes a NumPy matrix `X` and returns\n",
    "a new matrix with an additional column. The additional column should have all of its\n",
    "elements set to 1 and is located at the first column of the matrix.\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/add_bias.jpeg\" alt=\"adding bias\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 1: Example of a matrix before and after adding a bias column.</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Note**: Your function should work for all kinds of matrix shapes.\n",
    "\n",
    "**Hint**: Consider using `np.hstack` to add the bias column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_column(X):\n",
    "    '''\n",
    "    Create a bias column and combine it with X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (N, d) numpy matrix representing a feature matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        new_X (np.ndarray):\n",
    "            A (N, d + 1) numpy matrix with the first column consisting of all 1s\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    n = np.shape(X)[0]\n",
    "    new_col = np.ones((n, 1))\n",
    "    return np.column_stack((new_col, X))\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "without_bias = np.array([[1, 2], [3, 4]])\n",
    "expected = np.array([[1, 1, 2], [1, 3, 4]])\n",
    "\n",
    "assert np.array_equal(add_bias_column(without_bias), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Get best fitting bias and weights\n",
    "\n",
    "Write the function `get_bias_and_weight(X, y, include_bias)` that returns $w_0$ (bias) and\n",
    "$w_1, w_2, \\dots, w_n$ (weights) that will lead to best fitting line.\n",
    "\n",
    "The `include_bias` argument is used to specify if the model includes a bias term, i.e. has a non-zero bias term. Hence, the function should return $w_0 = 0$ if it is set to `false`. The function should return $w_1, \\dots, w_n$ as a NumPy matrix with shape $(n, 1)$, where $n$ is the number of features (excluding the bias column).\n",
    "\n",
    "We can use the normal equation to get $w_0, w_1, \\dots, w_n$. The normal equation is as\n",
    "follows:\n",
    "\n",
    "$$ \\begin{pmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix} = (X^TX)^{-1}X^T \\boldsymbol{y} $$\n",
    "\n",
    "where $X$ is the (augmented for bias) feature matrix and $\\boldsymbol{y}$ is the vector of target values.\n",
    "\n",
    "**Note**: You can use the `add_bias_column` function for this task. (You do not need to re-define the function in Coursemology. However, you are free to do so if you explicitly want to use your own implementation of the function for this task.)\n",
    "\n",
    "**Hint**: Consider using `numpy.linalg.inv` for the matrix inverse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_and_weight(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate bias and weights that give the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            If include_bias = True, return the bias constant. Else,\n",
    "            return 0\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix representing the weight constant(s).\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    Z = np.round(add_bias_column(X))\n",
    "    Y = np.round(y)\n",
    "\n",
    "    if include_bias:\n",
    "        w = np.linalg.inv(np.transpose(Z) @ Z) @ np.transpose(Z) @ Y\n",
    "        return (w[0, 0], w[1: ])\n",
    "    else :\n",
    "        w = np.linalg.inv(np.transpose(X) @ X) @ np.transpose(X) @ Y\n",
    "        return (0, w)\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 3. 4.]]\n",
      "(2.9999999999999645, array([[1.00000000e+00],\n",
      "       [1.77635684e-14]]))\n",
      "[[1. 1. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 3. 4.]]\n",
      "(2.9999999999999645, array([[1.00000000e+00],\n",
      "       [1.77635684e-14]]))\n",
      "[[1. 1. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 3. 4.]]\n",
      "(0, array([[0.48571429],\n",
      "       [1.2       ]]))\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "public_X, public_y = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "test_1 = (round(get_bias_and_weight(public_X, public_y)[0], 5) == 3)\n",
    "test_2 = np.array_equal(np.round(get_bias_and_weight(public_X, public_y)[1], 1), np.array([[1.0], [0.0]]))\n",
    "test_3 = np.array_equal(np.round(get_bias_and_weight(public_X, public_y, False)[1], 2), np.round(np.array([[0.49], [1.20]]), 2))\n",
    "\n",
    "assert test_1 and test_2 and test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_linear_regression(X, y, include_bias)` that returns `y_pred`,\n",
    "a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: You can use the `get_bias_and_weight` function for this task. (You do not need to re-define the function in Coursemology. However, you are free to do so if you explicitly want to use your own implementation of the function for this task.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_linear_regression(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        y_pred (np.ndarray):\n",
    "            A (m, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    bias, w = get_bias_and_weight(X, y, include_bias)\n",
    "    if include_bias:\n",
    "        X = add_bias_column(X)\n",
    "        w = np.vstack(([bias], w))\n",
    "    return X @ w\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_X, test_y = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "assert round(mean_squared_error(test_y, get_prediction_linear_regression(test_X, test_y)), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a linear regression model with **floor_area_sqm** as the only\n",
    "feature and **asking_price** as the target value. Plot your prediction line using the code\n",
    "snippet below. It should look similar to Figure 2.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/linear_reg.png\" alt=\"regression plot\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 2: Example of linear regression using <b>floor_area_sqm</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = X[:, 0].reshape((-1, 1))\n",
    "predicted = get_prediction_linear_regression(area, y)\n",
    "plt.scatter(area, y)\n",
    "plt.plot(area, predicted, color = 'r')\n",
    "plt.xlabel(\"Size in square meter\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We will now learn to use gradient descent to approximate $\\boldsymbol{w} = w_0, w_1, \\dots, w_n$.\n",
    "\n",
    "*Gradient descent*<sup>&#x2020;</sup> is an algorithm that minimizes the cost function by iteratively trying to\n",
    "find the best parameters. In linear regression, we will try to minimize the Mean Squared\n",
    "Error. The outline of the algorithm is as follows:\n",
    "    \n",
    "* Start with some $\\boldsymbol{w} = (w_0, \\dots, w_n)$\n",
    "* Keep changing $w_0,\\dots, w_n$ to minimize $J(\\boldsymbol{w})$, where $J$ is our cost function\n",
    "\n",
    "In this problem set, we will initially set $w_0, w_1, \\dots, w_n$ to all be 0s. Then, we will set a\n",
    "learning rate $\\alpha$ that will affect the rate of change of $w_0, \\dots, w_n$. Lastly, we will set\n",
    "$N$ to specify the number of epochs of gradient descent we want to run.\n",
    "\n",
    "The pseudo-code of Gradient Descent for linear regression is defined in Algorithm 1.\n",
    "\n",
    "**Note**: In the following gradient descent-related tasks, calculate the value of the loss function *after* updating the bias and weights.\n",
    "\n",
    "<sup>&#x2020;</sup> *The Gradient Descent algorithm is not limited to the linear regression model – it is a general optimisation technique and is also used in many other machine learning models such as Neural Networks.*\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"./images/mse_plot.png\" alt=\"gradient descent\" style=\"width: 45%; margin-right: 10px;\">\n",
    "    <img src=\"./images/grad_desc_algorithm.png\" alt=\"gradient descent\" style=\"width: 45%;\">\n",
    "</div>\n",
    "<figcaption style=\"text-align:left; margin-top: 10px;\">Figure 3: Gradient descent tries to find parameters that lead to the lowest MSE.</figcaption>\n",
    "\n",
    "For MSE, The partial derivative $\\frac{\\partial J(\\boldsymbol w)}{\\partial w_i}$ with $m$ training samples can be derived as: \n",
    "$$\n",
    "\\frac{\\partial J(\\boldsymbol{w})}{\\partial w_i} = \\frac 1m\\sum^m_{j=1}(h_w(x^{(j)})-y^{(j)})\\cdot x_i^{(j)} \n",
    "$$\n",
    "where $h_w$ is our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Gradient Descent on multiple features\n",
    "\n",
    "Write the function `gradient_descent_multi_variable(X, y, lr, number_of_epochs)` that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_n$ - $(n,1)$ NumPy matrix, where each element denotes the weight constant of a certain feature\n",
    "* $loss$ - a list that contains the MSE scores calculated during the gradient descent process.\n",
    "\n",
    "**Note**: You can use the `mean_squared_error` function for this task. (You do not need to re-define the function in Coursemology. However, you are free to do so if you explicitly want to use your own implementation of the function for this task.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250):\n",
    "    '''\n",
    "    Approximate bias and weight that gave the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    number_of_epochs (int) : Number of gradient descent epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix that specifies the weight constants.\n",
    "        loss (list):\n",
    "            A list where the i-th element denotes the MSE score at i-th epoch.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    loss = []\n",
    "    \n",
    "    N = X.shape[0]  \n",
    "    pred = X @ weights + bias\n",
    "    for e in range(number_of_epochs):\n",
    "        pred = X @ weights + bias\n",
    "        g_w = (1 / N) * (X.T @ (pred - y))\n",
    "        g_b = (1 / N) * np.sum(pred - y)\n",
    "        bias -= lr * g_b\n",
    "        weights -= lr * g_w\n",
    "        loss.append(mean_squared_error(y, pred))\n",
    "    return bias, weights, loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "_, _, loss = gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250)\n",
    "loss_initial = loss[0]\n",
    "loss_final = loss[-1]\n",
    "\n",
    "assert loss_initial > loss_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Which algorithm should we use for Linear Regression?\n",
    "\n",
    "Compare the pros and cons of using normal equation and gradient descent for linear regression. Specifically:\n",
    "\n",
    "- Compare the speed of the two algorithms on data with many features. \n",
    "- Compare the quality of the solutions obtained by the two algorithms. (i.e. how close to the optimal solution are the solutions obtained by the algorithms)\n",
    "- Compare whether feature scaling is necessary for each algorithm to perform well.\n",
    "\n",
    "Finally, select the algorithm you think is more suitable for this problemset and explain why you chose it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Polynomial Regression\n",
    "\n",
    "In real-world data, a straight line might not fit the data perfectly. Consider the relation between **schools** and **asking_price**.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/school_price_rel.png\" alt=\"school price relation\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 4: Schools - Price Relationship.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Houses with 0 schools nearby tend to be cheaper than houses with 1 school nearby. However, as the number of schools increases, the prices decrease. If we try a linear regression on the data, we obtain the following:\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/school_price_rel_linearfit.png\" alt=\"school price relation linear fit\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 4.1: Schools - Price Relationship With Linear Fit.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Notice how we lose the detail that houses with 0 schools are actually cheaper than houses with 1 school nearby. A polynomial function can better capture this relationship:\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/school_price_rel_cubicfit.png\" alt=\"school price relation cubic fit\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 4.2: Schools - Price Relationship With Cubic Fit.</figcaption>\n",
    "</figure>\n",
    "\n",
    "A polynomial function is written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n $$\n",
    "\n",
    "where $y$ is the target value, $x$ is a (*single*) feature value, and $n$ is the degree of the polynomial. $w_0$ is the bias term and $w_1, \\dots, w_n$ are the feature weights. \n",
    "\n",
    "Notice how if we set $x_1 = x, x_2 = x^2, \\dots, x_n = x^n$, then the polynomial function is simply linear regression with $n$ features:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 : Create Polynomial Matrix\n",
    "\n",
    "Write the function `create_polynomial_matrix(X, power)` that takes a $(m, 1)$-matrix and an\n",
    "integer, and returns a polynomial matrix with shape $(m, power)$.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{array}\\right]\n",
    "\\xrightarrow[]{\\text{create\\_polynomial\\_matrix(3)}}\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 1^2 & 1^3\\\\ \n",
    "2 & 2^2 & 2^3\\\\\n",
    "3 & 3^2 & 3^3\n",
    "\\end{array}\\right]\n",
    "\\rightarrow\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 1 & 1\\\\ \n",
    "2 & 4 & 8\\\\\n",
    "3 & 9 & 27\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "**Hint**: Consider using `np.tile`/`np.repeat` together with `np.cumprod`/`np.power`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_matrix(X, power = 2):\n",
    "    '''\n",
    "    Create a polynomial matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (m, 1) numpy matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, power) numpy matrix where the i-th column denotes\n",
    "            X raised to the power of i.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    return np.cumprod(np.tile(X, power), axis=1)\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "vector = np.array([[1], [2], [3]])\n",
    "poly_matrix = np.array([[1, 1, 1], [2, 4, 8], [3, 9, 27]])\n",
    "\n",
    "assert np.array_equal(create_polynomial_matrix(vector, 3), poly_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_poly_regression(X, y, power, include_bias)` that returns\n",
    "`y_pred`, a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: You can use the functions `create_polynomial_matrix` and `get_prediction_linear_regression` from before for this task. (You do not need to re-define the functions in Coursemology. However, you are free to do so if you explicitly want to use your own implementation of the functions for this task.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_poly_regression(X, y, power = 2, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best polynomial line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, 1) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    power (int) : Specify the degree of the polynomial\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    x = create_polynomial_matrix(X, power)\n",
    "    return get_prediction_linear_regression(x, y, include_bias)\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prediction_linear_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test cases\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_X, test_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction_poly_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mround\u001b[39m(mean_squared_error(test_y, pred_y), \u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36mget_prediction_poly_regression\u001b[0;34m(X, y, power, include_bias)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" YOUR CODE HERE \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m create_polynomial_matrix(X, power)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_prediction_linear_regression\u001b[49m(x, y, include_bias)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" YOUR CODE END HERE \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_prediction_linear_regression' is not defined"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "test_X, test_y = np.arange(3).reshape((-1, 1)), np.arange(4, 7).reshape((-1, 1))\n",
    "pred_y = get_prediction_poly_regression(test_X, test_y, 2)\n",
    "\n",
    "assert round(mean_squared_error(test_y, pred_y), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a polynomial regression model, using `power = 3` and `include_bias = True`, with **schools** as the only feature and **asking_price** as the target value. Plot your prediction line using the code snippet below. It should look similar to Figure 5.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"./images/poly_reg.png\" alt=\"polynomial regression\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\">Figure 5: Example of polynomial regression using <b>schools</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "schools = X[:, 2].reshape((-1, 1))\n",
    "predicted = get_prediction_poly_regression(schools, y, 3)\n",
    "plt.scatter(schools, y)\n",
    "plt.scatter(schools, predicted, color = 'r', s = 100)\n",
    "plt.xlabel(\"Number of schools within 1km\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Feature Scaling\n",
    "\n",
    "As we create a higher degree polynomial matrix, each column will have a larger scale\n",
    "than the previous one. This can lead to poor performance for gradient descent. Here\n",
    "is where feature scaling plays an important role. Write the function `feature_scaling(X)`\n",
    "that takes a NumPy matrix `X` and returns a mean-normalized matrix.\n",
    "\n",
    "**Note**: The normalization occurs on the column level (i-th column is normalized by the\n",
    "mean and standard deviation of the i-th column). That is,\n",
    "\n",
    "$$\n",
    "\\text{If} \\quad \\boldsymbol{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_k \\end{pmatrix} \\\\\n",
    "\\boldsymbol{v}_{norm} = \\frac{\\boldsymbol{v} - \\boldsymbol{\\hat{v}}}{\\sigma_{v}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{v}$ is a vector of $k$ elements, $\\boldsymbol{\\hat{v}}$ is its mean, and $\\sigma_{v}$ is its standard deviation.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 133\\\\\n",
    "4 & 700\\\\\n",
    "5 & 133\\\\\n",
    "8 & 700\n",
    "\\end{array}\\right]\n",
    "\\xrightarrow[]{\\text{feature\\_scaling}}\n",
    "\\left[\\begin{array}{cc} \n",
    "-1.4 & -1\\\\\n",
    "-0.2 & 1\\\\\n",
    "0.2 & -1\\\\\n",
    "1.4 & 1\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "\n",
    "Focusing on the first feature:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v} = \\left(\\begin{array}{cc}\n",
    "1\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "8\n",
    "\\end{array}\\right), \\boldsymbol{\\hat{v}} = 4.5, \\sigma_{v} = 2.5$$\n",
    "\n",
    "$$\\boldsymbol{v}_{norm} = \\frac{\\boldsymbol{v} - 4.5}{2.5} = \\left(\\begin{array}{cc}\n",
    "-1.4\\\\\n",
    "-0.2\\\\\n",
    "0.2\\\\\n",
    "1.4\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "**Hint**: Consider using `np.mean()` and `np.std()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    '''\n",
    "    Mean normalized each feature column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, n) numpy matrix where each column has been mean-normalized.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    return (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "public_X = np.array([[1, 133], [4, 700], [5, 133], [8, 700]])\n",
    "expected = np.array([[-1.4, -1], [-0.2, 1], [0.2, -1], [1.4, 1]])\n",
    "\n",
    "assert np.array_equal(feature_scaling(public_X), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Find number of epochs to converge\n",
    "\n",
    "Fill in the function `find_number_of_epochs(X, y, lr, delta_loss)` that that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_n$ - $(n, 1)$ NumPy matrix, where each element denotes the weight constant of a certain feature\n",
    "* $num\\_of\\_epochs$ - a number representing the number of epochs performed to reach convergence\n",
    "\n",
    "A single epoch is defined as performing the gradient descent *once* and calculating the loss. The loss calculation and gradient descent should be performed using MSE.\n",
    "\n",
    "The definition of convergence is as follows:\n",
    "\n",
    "$$ |J_{t-1} - J_{t}| < delta\\_loss $$\n",
    "\n",
    "where $J_{t-1}$ is loss at timestep $t-1$ (previous timestep), $J_{t}$ is loss at timestep $t$ (current timestep), and $delta\\_loss$ is the termination criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_of_epochs(X, y, lr, delta_loss):\n",
    "    '''\n",
    "    Do gradient descent until convergence and return number of epochs\n",
    "    required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    delta_loss (float) : Termination criterion\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix that specifies the weight constants.\n",
    "        num_of_epochs (int):\n",
    "            Number of epochs to reach convergence.\n",
    "        current_loss (float):\n",
    "            The loss value obtained after convergence.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    num_of_epochs = 0\n",
    "    previous_loss = np.inf\n",
    "    # current_loss = -1e14    \n",
    "\n",
    "    N = X.shape[0]\n",
    "    pred = X @ weights + bias\n",
    "    current_loss = mean_squared_error(y, pred)\n",
    "    while abs(previous_loss - current_loss) >= delta_loss:\n",
    "        previous_loss = current_loss\n",
    "        pred = X @ weights + bias\n",
    "        g_w = -(1 / N) * (X.T @ (y - pred))\n",
    "        g_b = -(1 / N) * np.sum(y - pred)\n",
    "        bias -= lr * g_b\n",
    "        weights -= lr * g_w\n",
    "        pred = X @ weights + bias\n",
    "        current_loss = mean_squared_error(y, pred)\n",
    "        num_of_epochs += 1\n",
    "    return bias, weights, num_of_epochs, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12496.906405098289, array([[21585.28999284],\n",
      "       [38708.26369539],\n",
      "       [-4515.94504127]]), 3953, 124453082703.73758)\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "poly_X = create_polynomial_matrix(X[:, 2].reshape((-1, 1)), 3)\n",
    "_, _, num_of_epochs, _ = find_number_of_epochs(poly_X, y, 1e-5, 1e7)\n",
    "print(find_number_of_epochs(poly_X, y, 1e-5, 1e7))\n",
    "assert num_of_epochs > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Analyze the effects of feature scaling on Gradient Descent\n",
    "\n",
    "In this task, we will examine the influence of feature scaling on the efficiency of gradient descent algorithms. Specifically, we utilize a degree 3 polynomial feature matrix derived from a dataset of schools.\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/loss_vs_epochs.png\" alt=\"loss against epochs with and without normalization\" width=\"100%\">\n",
    "</figure>\n",
    "\n",
    "The above figure shows plots of loss against epoch count, for gradient descent without and with normalization on the mentioned dataset. Focus on two key aspects:\n",
    "\n",
    "1. Convergence analysis\n",
    "    - Observe the number of epochs required to achieve convergence for both the non-normalized (original) and normalized feature matrices.  \n",
    "    This comparison should be conducted across various learning rates.\n",
    "2. Loss visualization\n",
    "    - Analyze the loss values corresponding to both the non-normalized and normalized matrices as a function of epoch count, again considering different learning rates.\n",
    "\n",
    "\n",
    "**Draw 2 observations or conclusions** from the above figures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 Part B: Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In class, we discussed about logistic regression, and how it can be useful as a classification algorithm. In Part B of this problem set, we get some hands-on practice by implementing logistic regression on a Credit Card Fraud Detection dataset. Note that for Part B, you should only be using the scikit-learn (sklearn) library for the last part (Task 6.1) on SVM.\n",
    "\n",
    "**Required Files**:\n",
    "\n",
    "* ps3.ipynb\n",
    "* credit_card.csv\n",
    "\n",
    "**Plagiarism Policy**: Please refer to our [Course Policies](https://canvas.nus.edu.sg/courses/62323/pages/course-policies)\n",
    "\n",
    "**IMPORTANT**: While it is possible to write and run Python code directly in Jupyter notebook, we recommend that you do this Problem set with an IDE using the .py file provided. An IDE will make debugging significantly easier.\n",
    "\n",
    "**Post-Problem Set Survey**:\n",
    "Your feedback is important to us! After completing Problem Set 3, please take a moment to share your thoughts by filling out this [survey](https://coursemology.org/courses/2851/surveys/2410)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "Similar to PS0, your implementation in the following tasks **should NOT involve any iteration, including `map` and `filter`, or recursion**. Instead, you should work with the operations available in NumPy. Solutions that violate this will be penalised.\n",
    "\n",
    "- You are allowed to use any mathematical functions, but this does not mean that you are allowed to use any NumPy function (there are NumPy functions that aren’t mathematical functions). For example, `np.vectorize` is not allowed since it is iterative. If you are in doubt about which functions are allowed, you should ask in the forum.\n",
    "\n",
    "There is, however, an exception for **Task 5.4**. In the pseudo-code for the algorithm required, there is an explicit for loop. Hence, only for this task you may use **a single for loop** to iterate for the number of epochs required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Fraudulent credit card transaction is a common phenomenon in many parts of the world and can lead to potentially large amounts of losses for both companies and customers. Therefore, we hope to help credit card companies recognize those fraudulent transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "We are given a dataset that contains transactions made by credit cards holders in `credit_card.csv`. If we think about what type of data might be included in the input variables under the given context, we might realize that those input variables are likely to include word descriptions, such as shop name or locations. In this problem set, we don't need to worry about language processing as the data are pre-processed to contain only numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Take a look at the columns in the dataset `credit_card.csv`. We have V1-V20, 'Amount', and 'Time' as input features, and 'Class' as output which takes the value 1 if it's fraud and 0 otherwise. This dataset presents 492 frauds out of 284,807 transactions. That means, there are 284,808 rows (including the header) in the csv file.\n",
    "\n",
    "We will use this dataset to implement logistic regression using stochastic gradient descent for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read credit card data into a Pandas dataframe for large tests\n",
    "\n",
    "dirname = os.getcwd()\n",
    "credit_card_data_filepath = os.path.join(dirname, 'credit_card.csv')\n",
    "\n",
    "credit_df = pd.read_csv(credit_card_data_filepath)\n",
    "X_task5 = credit_df.values[:, :-1]\n",
    "y_task5 = credit_df.values[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is an open source data analysis and manipulation tool in Python. In this problem set, we read the CSV into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to provide some nifty methods that makes it easier for us to handle large amounts of data. You can think of a dataframe as a large table that stores our dataset in a neat and optimized manner, making it fast for retrieval and manipulation and data. Using Pandas, we can quickly gain an overview of the type and values of the data stored, distributions of values within the dataset, and even ways to perform sampling.\n",
    "\n",
    "In the new few sections, we will explore some basic functions of Pandas to help us get started. You do not need to submit any codes for this section, and can simply run the cells to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [`head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method on the dataframe, we can get an overview of the data. By default, the method returns the first 5 entries in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can inspect the [value counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) of the 'Class' property in the dataframe to know the number of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the number of fraudulent and non-fraudulent transactions.\n",
    "credit_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and selecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NumPy, we can also index and select data on Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can [select columns in the dataframe by their labels](https://pandas.pydata.org/docs/user_guide/indexing.html#basics). In the following example, we use `'Class'` to index the 'Class' column in the credit dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'Class' column in the credit dataframe\n",
    "credit_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can use integer indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the first 2 rows\n",
    "credit_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also [select columns in the dataframe that fulfils some condition](https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing). In the example below, `credit_df['Class'] == 0` returns a [Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) of length 284807, which is the size of our dataset. It contains the value `True` if and only if the `Class` value for the particular entry is of value 0, and `False` otherwise. We can use this Boolean series to index the credit dataframe to return the rows where the `Class` field is 0. Does this remind you of how NumPy arrays operate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df['Class'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the credit dataframe where the 'Class' field is 0\n",
    "credit_df[credit_df['Class'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also [concatenate](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) Pandas series or dataframes together! In this example, we explored how we can concat the first 2 rows and last 2 rows of the dataframe. Similar to NumPy, you would also need to specify the axis for concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([credit_df[:2], credit_df[-2:]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "As you might have realised, managing Pandas is not so much different from how you would operate on NumPy or basic Python structures! In practice, Pandas can be very useful for data resampling (e.g. the [`sample` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Problem with imbalanced data\n",
    "\n",
    "Describe in one or two sentences what problem we might encounter if we directly use the given dataset without processing it.\n",
    "\n",
    "Hint: consider the 'Class' column, and think about how a high accuracy prediction can be achieved in a wrong way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept 4.2: Resampling methods\n",
    "\n",
    "When we are faced with the issue of imbalanced data, there are several ways to deal with it. A more direct way might be just to collect more data instances. We realized that in our case this doesn't work well because the events unevenly occur. We then look at how to resample the existing instances.\n",
    "\n",
    "In this problem set, you are introduced with three resampling methods: undersampling, oversampling, and SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 4.2.1: Undersampling\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/undersampling.png\" alt=\"visualisation of undersampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 6: Visualisation of undersampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates undersampling. In undersampling, we remove samples from the majority class. More specifically, we randomly take subsamples of the majority class such that the size of two classes is the same. There is, however, a drawback of undersampling - by removing data randomly, you might have removed some valuable information from the dataset. Is there any way to do better without losing the valuable information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 4.2.2: Oversampling\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/oversampling.png\" alt=\"visualisation of oversampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 7: Visualisation of oversampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates oversampling. In oversampling, we duplicate records of the minority class such that the size of two classes balance out. There is a bright side of oversampling - you don't lose certain valuable information. At the same time, you also realize the drawback of oversampling - it can cause overfitting and a poor generalization of the test set. Now the question is, instead of simply duplicating records, is there any other way to increase the number of records in the minority set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 4.2.3: SMOTE (for further reading only)\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/smote.png\" alt=\"visualisation of SMOTE\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 8: Visualisation of SMOTE.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates Synthetic Minority Oversampling Technique (SMOTE).\n",
    "\n",
    "The SMOTE algorithm works in these four steps:\n",
    "\n",
    "1. Consider minority and majority instances in vector space.\n",
    "1. For each minority-class instance pair, interpolate their feature values.\n",
    "1. Randomly synthesize instances and label with minority class\n",
    "1. More instances added to minority class\n",
    "\n",
    "In this case, you increase the number of minority instances without simply duplicating the values. Now your newly inserted minority record is not an exact copy of an existing data point, but it is also not too different from the known observations in your minority class. Outside this problem set, when we want to do data resampling, we can use the [Python imbalanced-learn library](https://pypi.org/project/imbalanced-learn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word of caution when resampling\n",
    "\n",
    "In the section above, we introduced three resampling techniques. One very important thing to note is, in practice you should first split dataset to train–test sets, then resample the train dataset before training the model. This is done to avoid data leakage (snooping).\n",
    "\n",
    "Data leakage can cause you to create overly optimistic if not completely invalid predictive models. It is when information from outside the training dataset is used to train the model. The additional information might allow the model to learn something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "Before we can start training our models, we need to **randomly** partition our dataset into training data and testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Splitting data into train and test sets\n",
    "\n",
    "Remember that we are trying to make a model that can predict fraudulence of data points that the model has never seen. It would be unwise to measure the accuracy of the model using the same data it trained on, as that does not give you an idea of how well the model will work on unseen data. Hence, we will have to train the model on a training set and evaluate its effectiveness (e.g. using metrics such as accuracy, F1 score etc.) on a separate test set. The [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) library provides methods to perform a [train-test data split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Correspondingly, if you intend to do more with your model, for example, fine-tune it by adjust various hyperparameters or compare it with other models, you are advised to use separate development or validation sets respectively (which are disjoint from the training and test sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Recall the gradient descent method you learned and applied in the previous problem set. Gradient descent is an iterative optimization method which finds the minimum of a differentiable function. It's often used to find the coefficients that minimize the cost function. Here is a brief recap of how that's done:\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/gradient_descent.png\" alt=\"visualisation of gradient descent\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\"><a href=\"https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1\">Figure 9: Visualisation of gradient descent.</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "We start at an initial weight vector $\\mathbf{w} = (w_0, w_1, ..., w_n)$. Then we take incremental steps along the steepest slope to get to the bottom where the minimum cost lies. Along the way, we keep updating the weight coefficients by computing the gradients using the training samples from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Batch gradient descent\n",
    "\n",
    "In batch gradient descent, **all** the training data (entire batch) is taken into consideration in a single step. For one step of gradient descent, we calculate $w_j \\leftarrow w_j - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j}$ (where $\\alpha$ is the learning rate) simultaneously for all $j$ based on all the training examples and then use that mean gradient to update our parameters.\n",
    "\n",
    "By continuing this for iterations until convergence, or until it's time to stop, we reach the optimal or near optimal parameters $\\mathbf{w}$ leading to minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Stochastic gradient descent\n",
    "\n",
    "Think about what's a limitation of the (entire) batch gradient descent introduced earlier. What if the dataset is very large? In Stochastic Gradient Descent (SGD), we consider only the error on a single record $(x^{(*)}, y^{(*)})$ at a time. How does this work?\n",
    "\n",
    "During the training loop, we will randomly sample an instance in the dataset. This instance will be used to update the weights of the model $w_j \\leftarrow w_j - \\alpha \\frac{\\partial J_{stoch}(\\mathbf{w})}{\\partial w_j}$ (where $\\alpha$ is the learning rate) simultaneously for all $j$.\n",
    "\n",
    "Note that SGD does not necessarily decrease the batch loss in each iteration! However, on average, the loss will still decrease. Since we work on only a single record at any time, SGD provides the advantage of a much smaller memory requirement, and faster computation time. SGD also comes with its own set of disadvantages, for example, we lose benefits of any vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Gradient descent termination condition\n",
    "\n",
    "The next question is when to stop? The ideal case is to run until convergence, but convergence might be hard to obtain. Here are some criterias you can use:\n",
    "\n",
    "* Stop when error change is small and/or\n",
    "* Stop when error is small\n",
    "* Stop when maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to apply what we learned about batch gradient descent and stochastic gradient descent.\n",
    "Task 5.1 to 5.5 will gradually guide you to complete the logistic regression using stochastic gradient descent. For the task in this problem set, you can assume that the bias column has been added for all input `X`. That means given `X` as an argument, you don't need to manually add the bias column again in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Cost function\n",
    "\n",
    "Recall that for logistic regression, we want an error function for an individual value to be:\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/piecewise_error.png\" alt=\"error function\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 10: Cost function.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can simplify the condition and transform it into the equivalent cost function:\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/error.png\" alt=\"error function as a line\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 11: Cost without conditions.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In this task, you need to implement `cost_function` $E$ as mentioned above. This function takes `X`, `y`, and `weight_vector` $\\mathbf{w}$ as arguments, and returns the error $E$. Note that for this task, the $E$ should account for **all** the training data.\n",
    "\n",
    "Here, we are using the $\\log$ function and we need to handle the case of computing $\\log(0)$. There are many ways to handle this. In this task, we will handle $\\log(0)$ by using the machine epsilon for numpy `float64` type, and use the trick $\\log(x + eps)$ which allows $x$ to be $0$. If $x$ was any other value, $\\log(x + eps)$ would be very close to $\\log(x)$. This helps to improve numerical stability in computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X: np.ndarray, y: np.ndarray, weight_vector: np.ndarray):\n",
    "    '''\n",
    "    Cross entropy error for logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Cost\n",
    "    '''\n",
    "    \n",
    "    # Machine epsilon for numpy `float64` type\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1], [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([0.002, 0.1220])\n",
    "\n",
    "assert np.round(cost_function(X1, y1, w1), 5) == np.round(1.29333, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Weight update\n",
    "\n",
    "In this task, you need to implement `weight_update`. This function takes `X`, `y`, `alpha`, and a `weight_vector` as arguments, and output the new weight vector. Each call to the function should make one update on the weight vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(X: np.ndarray, y: np.ndarray, alpha: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.1\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New weight vector after one round of update.\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],[111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([2.2000, 12.20000])\n",
    "a1 = 1e-5\n",
    "nw1 = np.array([2.199,12.2])\n",
    "\n",
    "assert np.array_equal(np.round(weight_update(X1, y1, a1, w1), 3), nw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Logistic regression classification\n",
    "\n",
    "Remember that logistic regression is used for classification even though the function gives a probability output. In this task, you classify each element in `X`, given `weight_vector` using `prob_threshold` as the threshold, and output the classification result as an `np.ndarray`.\n",
    "\n",
    "If the probability predicted by the `weight_vector` exceeds the `prob_threshold`, we should classify it as fraud (`y = 1`). Otherwise, we should classify it as legitimate (`y = 0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classification(X: np.ndarray, weight_vector: np.ndarray, prob_threshold: np.float64=0.5):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight parameters.\n",
    "    prob_threshold: np.float64\n",
    "        the threshold for a prediction to be considered fraudulent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (m,) np.ndarray\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([-0.000002, 0.000003])\n",
    "expected1 = np.transpose([0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "result1 = logistic_regression_classification(X1, w1)\n",
    "\n",
    "assert result1.shape == expected1.shape and (result1 == expected1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.4: Logistic regression using stochastic gradient descent\n",
    "\n",
    "In this task, you need to implement logistic regression using stochastic gradient descent. This function takes `X_train`, `y_train`, `max_num_iterations`, `threshold`, `alpha`, and `seed` as arguments, and output the final `weight_vector` you obtained. You can make use of the previous functions you have implemented earlier.\n",
    "\n",
    "For stochastic gradient descent, an epoch refers to one pass through **one randomly selected data point**. Hence, one iteration of gradient descent, which updates the weight vector based on the cost function computed on the randomly selected data point, corresponds to one epoch. Stochastic gradient descent is terminated when the number of update rounds exceeds the maximum number of epochs (`max_num_iterations`) given, or when the computed error decreases to or below the threshold value (`threshold`).\n",
    "\n",
    "For this task, you should use `np.random.choice` to randomly choose the training data for each iteration. You should also use the `seed` argument with `np.random.seed(seed)` to set the random seed for reproducibility.\n",
    "\n",
    "For your convenience, you may define your own `weight_update_stochastic` method, which does the weight update for one step in stochastic gradient descent, inside the `logistic_regression_stochastic_gradient_descent` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_iterations: int=250, threshold: np.float64=0.05, alpha: np.float64=1e-5, seed: int=43) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write a terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    max_num_iterations: int\n",
    "        this should be one of the terminating conditions. \n",
    "        The gradient descent step should happen at most max_num_iterations times.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    seed: int\n",
    "        seed for random number generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight parameters\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = cost_function(X1, y1, np.transpose(np.zeros(X1.shape[1])))\n",
    "\n",
    "assert cost_function(X1, y1, logistic_regression_stochastic_gradient_descent(X1, y1)) < expected1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.5: Stochastic gradient descent vs batch gradient descent\n",
    "\n",
    "Recall that it is ideal for gradient descent to run until convergence, but convergence might be hard to obtain.\n",
    "\n",
    "In this task, we will explore the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent and stochastic gradient descent using a sample of the `credit_card` dataset.\n",
    "\n",
    "Upon measuring the runtime of batch and stochastic gradient descent for different epoch counts, we get the following graphs:\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/batch_vs_stochastic_update_rounds.png\" alt=\"CE loss against epoch count\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 12: cross entropy loss vs the number of update rounds for batch and stochastic gradient descent.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/batch_vs_stochastic_runtime.png\" alt=\"CE loss against runtime\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 13: cross entropy loss vs runtime for batch and stochastic gradient descent.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Do note that the runtime is the cumulative runtime of the entire run of gradient descent, not the runtime per individual update round. \n",
    "\n",
    "With reference to figures 12 and 13, what do you observe about the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent and stochastic gradient descent?\n",
    "\n",
    "Hint: You may/may not angle your answer in terms of explaining the effect of the size of the dataset, whether the algorithm will be stuck in a local minima and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "Now let's apply the support vector machine we learnt in lecture to the credit card dataset. Here is a quick recap of what SVM is about: We are given a training dataset of $n$ points of the form $(x^{(i)}, y^{(i)})$, where the $y^{(i)}$ are labels, each indicating the class to which the point $x^{(i)}$ belongs. Typically, the SVM is formulated with $y^{(i)}$ being either 1 or -1, however the following discussion uses equivalently $y^{(i)}$ being either 1 or 0. Be careful that some examples in this problem use data with 1,0 and some examples use data with 1,-1. \n",
    "\n",
    "Each $x^{(i)}$ $n$-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points $x^{(i)}$ for which $y_{i}=1$ from the group of points for which $y_{i}=0$, which is defined so that the distance between the hyperplane and the nearest point $x^{(i)}$ from either group is maximized.\n",
    "\n",
    "<figure>\n",
    "<img src=\"./images/svm.png\" alt=\"visualisation of support vector machine\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 14: Visualisation of support vector machine.</figcaption>\n",
    "</figure>\n",
    "\n",
    "How can we do construct such a \"maximum-margin hyperplane\"? Recall that in the lecture, we formulated soft-margin SVMs as the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w} C \\left[\\sum_{i = 1}^{m} y^{(i)}\\ Cost_1(\\mathbf{w}^{T} \\mathbf{x}^{(i)}) + (1 - y^{(i)})\\ Cost_0(\\mathbf{w}^{T} \\mathbf{x}^{(i)})\\right] + \\frac{1}{2} \\sum_{i = 1}^{n} \\mathbf{w}_i^2 $$\n",
    "\n",
    "where $Cost_1(\\mathbf{w}^T \\mathbf{x}^{(i)}) = \\max(0, 1 - \\mathbf{w}^T \\mathbf{x}^{(i)})$ and $Cost_0(\\mathbf{w}^T \\mathbf{x}^{(i)}) = \\max(0, 1 + \\mathbf{w}^T \\mathbf{x}^{(i)})$, with the hypothesis function $h_\\mathbf{w}(\\mathbf{w}) = 1$ if $\\mathbf{w}^T \\mathbf{x} \\geq 0$, and 0 otherwise.\n",
    "\n",
    "In particular, the summand term is known as the *hinge loss*. Thus, when $y^{(i)} = 0$, the more negative $\\mathbf{w}^T \\mathbf{x}$ is, the more $Cost_1$ penalises the model. Similarly, $Cost_0$ penalises the model the further $\\mathbf{w}^T \\mathbf{x}$ strays from the negative region. Notice how the cost function aligns with the goal of the hypothesis function? Additional note: $Cost_0 = Cost_{-1}$, where $Cost_{-1}$ was used in the lecture. \n",
    "\n",
    "Furthermore, we also add a regularization parameter $C$ to the cost function to balance the trade-off between margin maximization and loss.\n",
    "\n",
    "Thus, solving this minimization problem allows us to find the weight vector $\\mathbf{w}$ that provides the maximum-margin hyperplane while balancing the trade-off of overfitting.\n",
    "\n",
    "With this set up in mind, how can we implement this in code? Thankfully, with modern machine learning libraries, much of the tedious work of solving this minimization problem has been done. As machine learning practitioners, one popular library that we commonly use (aside from [NumPy](https://numpy.org/doc/stable/index.html)) is [scikit-learn](https://scikit-learn.org/), which is built on top of NumPy and provides simple and efficient tools to implement common machine learning algorithms.\n",
    "\n",
    "For the following task, we will focus on comparing Linear and Gaussian Kernel SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Linear SVM vs Gaussian Kernel SVM\n",
    "\n",
    "In this task, the code for `linear_svm` and `gaussian_kernel_svm` has been given to you. These functions take in `X` and `y` (the entire dataset), do a train test data split with `test_size` of 0.3 and `random_state` of 42, create an instance of a Linear/Gaussian kernel SVM classifier using the default parameters, train the classifier, and output the predictions as well as the accuracy score. Run the code cells below (you may modify them if you wish).\n",
    "\n",
    "Based on your observations, when using support vector machines, how do you think we should choose between linear kernel vs Gaussian kernel? In other words, which kernel is better in what cases? Give 2 reasons why one might be more suitable than the other in different cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(X: np.ndarray, y: np.ndarray):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    clf_predictions = clf.predict(X_test)\n",
    "\n",
    "    return clf_predictions, clf.score(X_test, y_test) * 100\n",
    "\n",
    "\n",
    "# small data\n",
    "# Do note that y values for data1 are either 0 or 1 for this half of the task, but typically are \n",
    "#   either -1 or 1 for SVMs. You do not have to change this data for this task.\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "result1 = linear_svm(X1, y1)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X_task6 = data_100.iloc[:, :-1].to_numpy()\n",
    "y_task6 = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "result = linear_svm(X_task6, y_task6.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_svm(X: np.ndarray, y: np.ndarray):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "    gaussian_kernel_classifier = svm.SVC(kernel='rbf')\n",
    "    gaussian_kernel_classifier.fit(X_train, y_train)\n",
    "\n",
    "    gaussian_kernel_classifier_predictions = gaussian_kernel_classifier.predict(X_test)\n",
    "\n",
    "    return gaussian_kernel_classifier_predictions, gaussian_kernel_classifier.score(X_test, y_test) * 100\n",
    "\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, -1], [111.2, 20, -1], [111.3, 10, -1], [111.4, 10, -1], [111.5, 10, -1], [211.6, 80, 1],\n",
    "        [111.4, 10, -1], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "result1 = gaussian_kernel_svm(X1, y1)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X_task6 = data_100.iloc[:, :-1].to_numpy()\n",
    "y_task6 = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "result = gaussian_kernel_svm(X_task6, y_task6.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right snippets of code into the corresponding box that says \"Your answer,\"and click \"Save.\" After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click \"Finalize submission.\" Note that once your submission is finalized, it is considered to be submitted for grading and cannot be changed. If you need to undo this action, you will have to email your assigned tutor for help. Please do not finalize your submission until you are sure that you want to submit your solutions for grading.\n",
    "\n",
    "*Have fun and enjoy coding.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
