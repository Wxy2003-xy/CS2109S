\documentclass{article}
\usepackage[utf8]{inputenc}    
\usepackage[T1]{fontenc}       
\usepackage{lmodern}           
\usepackage{amsmath}   
\usepackage{amssymb}   
\usepackage{geometry}  
\usepackage{enumerate} 
\usepackage{xcolor}  
\usepackage{amsthm}
\usepackage{pdfpages}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{system}%
{\left\lbrace\begin{array}{@{}l@{}}}%
{\end{array}\right.}
\usepackage{listings}  
\usepackage{dsfont}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,   
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{brown},
  stringstyle=\color{orange},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\geometry{top=0.3in, bottom=0.3in, left=0.3in, right=0.3in}


\begin{document}

\title{}
\author{Wang Xiyu}
\date{}
\maketitle
\section{Overview}
\begin{itemize}
    \item Week 1-3: Classical AI, search algorithms
        \begin{enumerate} 
            \item Uninformed search
            \item Local search: hill climbing
            \item Informaed search: A$^*$
            \item Adversarial search Minimax
        \end{enumerate}
    \item Week 4-7: Classical ML
        \begin{enumerate}
            \item Decision trees 
            \item Linear/Logistic regression 
            \item Kernels and support vector machines
            \item "Classical" unsuperivese learning
        \end{enumerate}
    \item Week 10-12: Modern ML
        \begin{enumerate}
            \item Neural networks
            \item Deep learning 
            \item Sequential data
        \end{enumerate}
    \item Week 13: Misc.
\end{itemize}

\section{AI: Computers Trying to Behave Like Humans}

\begin{itemize}
    \item \textbf{PEAS Framework:}
    \begin{itemize}
        \item \textbf{Performance measure:} define “goodness” of a solution
        \item \textbf{Environment:} define what the agent can and cannot do
        \item \textbf{Actuators:} outputs
        \item \textbf{Sensors:} inputs
    \end{itemize}
    
    \item Agent function is sufficient.
    
    \item Common agent structures (to define an AI agent):
    \begin{itemize}
        \item Reflex
        \item Goal-based
        \item Utility-based
        \item Learning
        \item (Others possible; can mix and match!)
    \end{itemize}
    
    \item Exploration vs exploitation
\end{itemize}
\section{Problem Statement}
fully observable $\land$ deterministic $\land$ static $\land$ discrete $\implies$ only need to observe once \newline
To solve a prob using search: 
\begin{itemize}
    \item A goal or a set of goals
    \item a model of the enironment  
    \item a search algorithm 
\end{itemize}

goal formulation -> problem formulation -> search -> execute \newline
\begin{enumerate}
    \item goal formulation
    \item problem formulation, eg. path finding 
        \begin{itemize}
            \item states: nodes representation invariant:: abstract states should correspond to concrete states
            \item initial state: starting node
            \item goal states/test: dest node \newline
                Goal test: define the goal using a function $is\_goal $
            \item actions: move along an edge :: $|actions(state)| \leq branching\_factor$
            \item transition model: 
                $f(curr\_state, action) \implies next\_state $
            \item action cost function: see edges
        \end{itemize}
    \item Important facts:
        \begin{itemize}
            \item Representation Invariant: ensure that the abstract states correspond to concrete states
            \item Goal Test: Goal defined via a function $is\_goal$
            \item Action: a set of $action(state)$, $|actions(state)| \leq branching\_factor$
            \item Transition model: $f(curr\_state, action) \implies next\_state $
        \end{itemize}
\end{enumerate}



\section*{Search}

\subsection*{Uninformed search} 
No information that could guide the seaech: no clue how good a state is
\begin{lstlisting}
    create frontier 
    // create visited // with vsited memory
    insert Node(initial_state) to frontier 
    while frontier is not empty: 
        node = frontier.pop() 
        if node.state is goal: 
            return solution 
    //  if node.state in visited: // with vsited memory
    //     continue 
    //  visited.add(state)
        for action in actions(node.state): 
        next_state = transition(node.state, action) 
        frontier.add(Node(next_state)) 
    return failure
\end{lstlisting}
Different subvariant of tree search uses differen DS for the frontier.\newline
\begin{tabular}{|l|l|l|}
    \hline
    \textbf{Search Type} & \textbf{Data Structure for Frontier}\\
    \hline
    BFS & Queue \\
    \hline
    DFS & Stack \\
    \hline
    UCS (Uniform-cost Search) & Priority Queue \\
    \hline
\end{tabular}

    
\subsection*{Depth limited search} 
limit the search to depth l \newline
backtrack when the limit is hit. \newline
time complexity: exponential to search depth \newline
space complexity: size of the frontier \newline
\begin{lstlisting}
    create frontier 
    tier = 0
    insert Node(initial_state) to frontier 
    while (!empty(frontier)) &&  (tier <= limit):
        node = frontier.pop() 
        tier++
        if node.state is goal: 
            return solution 
        for action in actions(node.state): 
        next_state = transition(node.state, action) 
        frontier.add(Node(next_state)) 
    return failure
\end{lstlisting}
\subsection*{Iterative deeptening search} 
search with depth from 0 to inf \newline
return soln when found. Both complete 
\begin{lstlisting}
    create frontier 
    tier = 0
    insert Node(initial_state) to frontier 
    while (!empty(frontier)) && (tier <= limit):
        node = frontier.pop() 
        tier++
        if node.state is goal: 
            return solution 
        for action in actions(node.state): 
        next_state = transition(node.state, action) 
        frontier.add(Node(next_state)) 
    return failure
\end{lstlisting}
\subsection*{Summary} 
\begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \textbf{Name}                  & \textbf{Time Complexity*} & \textbf{Space Complexity*} & \textbf{Complete?} & \textbf{Optimal?} \\
    \hline
    Breadth-first Search           & Exponential               & Exponential                 & Yes                & Yes              \\
    \hline
    Uniform-cost Search            & Exponential               & Exponential                 & Yes                & Yes              \\
    \hline
    Depth-first Search             & Exponential               & Polynomial                  & No\#                 & No           \\
    \hline
    Depth-limited Search(DFS)          & Exponential               & Polynomial               & No               & No           \\
    \hline
    Depth-limited Search(BFS)           & Exponential               & Exponential                & No               & Yes           \\
    \hline
    Iterative Deepening Search(DFS)     & Exponential               & Exponential               & Yes                & Yes              \\
    \hline
    Iterative Deepening Search(BFS)     & Exponential               & Exponential               & Yes                & Yes              \\
    \hline
\end{tabular}
\newline
\# Not complete if not tracking visited nodes, search may stuck in loop before visiting all nodes. \newline
* In terms of some notion of depth/tier \newline




\section{Local Search}
Systematic search: typically complete and optimal under certain constraints. However intractable sometimes\newline
Local search: typically incomplete and suboptimal, but has anytime property, ie. longer time -> better solution. Able to provide good enough solution under reasonable amount of time. \newline


\subsection{}
\begin{enumerate}
    \item Start at random position in the state space
    \item iteratively move from a state to another neighouuring state vie perturbation or construction 
    \item solution is the final state 
\end{enumerate}

\begin{equation}
    \text{State space: all possible configuration}
\end{equation}
\begin{equation}
    \text{Search space: a subset of state space that will be explored}
\end{equation}

\subsubsection{Perturbation search}
\begin{itemize}
    \item Search space: complete candidate solutons
    \item search step: modification of one or more solution 
\end{itemize}
For example: swap a path with another path \newline
\subsubsection{Constructive search}
\begin{itemize}
    \item partial candidate soluton
    \item extension of one or more solution
\end{itemize}
For example: path finding\newline


\subsection{}
goal formulation -> problem formulation -> search -> execute \newline
\begin{enumerate}
    \item goal formulation
    \item problem formulation, eg. path finding 
        \begin{itemize}
            \item states: nodes representation invariant:: abstract states MAYNOT directly correspond to concrete states
            \item initial state: starting node, a candidate solution
            \item goal states/test: dest node [optional]\newline
                Goal test: define the goal using a function $is\_goal $
                $f(curr\_state, action) \implies next\_state $
            \item Successor function: a funciton that generates neighbouring states by applying modifications from the curren tstate. This defines the local search space 
            
        \end{itemize}
\end{enumerate}
\subsection{Evaluation function}
A math function that assess the quality or desireability of the solution. \newline
Some solutions may be unacceptable but there are some less bad than the others. 
\subsection{Hill Climbing/ Greedy Local Search}
\begin{lstlisting}
    curr_state = init_state
    while 1:
        best_succ = best(successor(curr_state))
        if (eval(best) <= eval(curr_state))
            return curr_state
        curr_state = best_succ
\end{lstlisting}

\subsection{State space landscape}
\begin{itemize}
    \item Global max: 
    \item Local max:
    \item shoulder: 
\end{itemize}

\section{Heuristics}
\subsection*{Admissibility}
A heuristic function \( h(n) \) is \textbf{admissible} if it \textbf{never overestimates} the true cost \( h^*(n) \) of reaching the goal state. Formally, for all nodes \( n \):

\[
h(n) \leq h^*(n)
\]

where:
\begin{itemize}
    \item \( h(n) \) is the estimated cost from node \( n \) to the goal.
    \item \( h^*(n) \) is the true minimum cost from \( n \) to the goal.
\end{itemize}

An admissible heuristic ensures that an A\(^*\) search using it will always find an \textbf{optimal} solution.

\subsection*{Consistency (Monotonicity)}
A heuristic \( h(n) \) is \textbf{consistent} (or \textbf{monotonic}) if, for every node \( n \) and every successor node \( n' \) reached via action \( a \), the estimated heuristic cost satisfies:

\[
h(n) \leq h(n') + c(n, n')
\]

where:
\begin{itemize}
    \item \( c(n, n') \) is the actual cost of moving from \( n \) to \( n' \).
\end{itemize}

Consistency implies that the estimated cost of a node never increases by more than the cost of an actual move, ensuring that the total estimated cost \( f(n) = g(n) + h(n) \) is always non-decreasing along a path.

\subsection*{Relationship Between Admissibility and Consistency}
\begin{itemize}
    \item If a heuristic is \textbf{consistent}, it is also \textbf{admissible}.
    \item If a heuristic is \textbf{admissible}, it is \textbf{not necessarily consistent}.
\end{itemize}
\subsection*{Prove: Consistency implies admissibility}
Admissibility
\begin{equation}
    \forall n \in State, h(n) \leq \sum_{i = start}^{goal}c(i', a, i)
\end{equation}
Consistency
\begin{equation}
    \forall n \in State, \forall a \in Action, h(n) \leq h(n') + c(n', a, n)
\end{equation}
\begin{proof}
    Base case: $n_{start} \vdash n_{goal}$, $h(n_{goal}) = h^*(n_{goal})$, 
    \[h(n_{start}) \leq c(n_{start}, n_{goal}) + h^*(n_{goal}) = h^*(n_{start})\]
    Inductive case: suppose n is of k distance away from $n_{goal}$ on the optimal path from $n_{start}$. From the base case we have, 
    \[h(n_k) \leq h^*(n_k)\]
    \[h(n_{k-1}) \leq h(n_k) + c(n_{k-1}, n_k) \leq h^*(n_k) + c(n_{k-1}, n_k) = h^*(n_{k-1})\]


\end{proof}



\section{Adversarial search}
\subsection{Classical adversarial games}
\begin{itemize}
    \item Fully observable
    \item Deterministic
    \item discrete
    \item No infinite run
    \item 2-player zero-sum
    \item turn taking
\end{itemize}
\subsection*{termns}
\begin{itemize}
    \item Player: agent
    \item Turn: 
    \item Move
    \item End state
    \item winning conditon
    \item 
\end{itemize}
\subsection{Problem formulation in adversarial search}
        \begin{itemize}
            \item states: nodes representation invariant:: abstract states MAYNOT directly correspond to concrete states
            \item initial state: starting node, a candidate solution
            \item Terminal State: state the outcome of the game when it terminates
            \item Utility function: output the value of a state from the perspetive of our agent 
        \end{itemize}

\subsection{Minimax}
In the view of A, A try to maximize the outcome of the game, B will try to minimize A's outcome, as the gam is zero sum\newline
\begin{itemize}
    \item expend(state) => [a]
    

\end{itemize}
\begin{lstlisting}[mathescape=true]
    max_value(state):
        if is_terminal(state): return utility(state)
        v = -$\infty$
        for next_state in expand(state):
            v = max(v, min value for player A in next_state)
        return v
    
    min_value(state):
        if is_terminal(state): return utility(state)
        v = $\infty$
        for next_state in expand(state):
            v = min(v, max value for player A in next_state)
        return v

    minimax(state): 
        v = max_value(state)
        return action in expand(state) with value v
\end{lstlisting}

\subsection{Alpha-beta prunning}
From the viewpoint of the MAX player:
\begin{itemize}
    \item $\alpha$: the value of the best choice for the MAX player so far
    \item $\beta$: the value of the best choice for the MIN player so far
\end{itemize}
An optimized version of the Minimax algorithm using prunning:
\begin{lstlisting}[mathescape=true]
    max_value(state, $\alpha$, $\beta$):
        if is_terminal(state): return utility(state)
        v = -$\infty$
        for next_state in expand(state):
            v = max(v, min(v, $\alpha$, $\beta$))
        return v
    
    min_value(state, $\alpha$, $\beta$):
        if is_terminal(state): return utility(state)
        v = $\infty$
        for next_state in expand(state):
            v = min(v, max(v, $\alpha$, $\beta$))
        return v

    alpha-beta search(state): 
        v = max_value(state, -$\infty$, $\infty$) // initialized $\alpha$ to be -$\infty$, $\beta$ to $\infty$
        return action in expand(state) with value v
\end{lstlisting}















\section{Learning agent }
For problems that the function are difficult to specify, solutions re intractale to compute in general. Typically episodic, 
\[DL\subset ML\subset AL\]
\subsection{supervised}
Learn the mapping input, feedback given -> output, given a dataset, 
it minimizes the difference betwenn the prediction and the provided correct ans
using a leanign algorithm  e.g. image ientification
\begin{enumerate}
    \item Train phase: try minimizing the diff between pred and correct ans given using the training set, 
    resulting in a trainign agen function, known as the model/hypothesis
    \item Testing/evaluation phase: using a test set to measure the performance of the model. 
    The performance on unseen data measures the generalization of the model 
\end{enumerate}



\subsubsection*{Task}
\begin{itemize}
    \item Classification: to predict discreate labels or catagories on the input features
    \item Regression: predict continuous numetical value based on input features
\end{itemize}
\subsubsection*{Dataset}
\[D = \bigcup_{[1, n]}\{ (x^{(i)}, y^{(i)})\}\]
\subsubsection*{True data generation function}
\[y = f^*(x) + \epsilon\]
where $f^*(x)$ is true but unknow, which generates the label from the input features; $\epsilon$ is some noise or error term, which account for the randomness or imperfection in the date generating process. \newline
the goal is to find a function that best approximately $f^*(x)$
\subsubsection*{Hypothesis class}
THE set of models or functions that maps from inputs to outputs $h:X \implies Y$ that can be learned by a learing algorithm. 
Each element of the hypo clas $h \in \mathcal{H}$ 
\subsubsection*{LEarnign algorithm }
\[A(D_{train}, H_{hypo}) = h(x),  h\in \mathcal{H} \approx f^*(x)\]
\subsubsection*{PErformance measure}
\[h(x) \approx f^*(x)\]
\[PM(D_{test}, h \in H_{hypo}) \mapsto \]
Try the hypothesis h on a new set of examples (test data)\\

\subsubsection*{Regression: error}
\begin{equation}
    \text{Absolute error} = |\hat{y} - y|
\end{equation}
\begin{equation}
    \text{Squared error} = (\hat{y} - y)^2 
\end{equation}
Mean squared error
\[MSE = \frac{1}{N} \sum_{i=0}^{N}(\hat{y}^{(i)} - y^{(i)})^2 \]
Mean absolute error
\[MAE = \frac{1}{N} \sum_{i=0}^{N}|\hat{y}^{(i)} - y^{(i)}| \]
where 
\[\hat{y}^{(i)} = h(x^{(i)})\]
Accuracy: 
\[A = \frac{1}{N} \sum_{i=1}^{n} \mathds{1}_{\hat{y}^{(i)} = y^{(i)}}\]



\subsubsection*{confusion matrix}
True positive, false positive, false negatve, true negative (2, 1, 3, 4 quadrnt)\newline
\[Accuracy = \frac{TP + TN}{TP + FN + FP + TN}\]
\[Precision = \frac{TP}{(TP + FP)}\]
maximize if FP is costly
\[Recall = \frac{TP}{(TP + FN)}\]
maximize recall if FN is dangerous
\[F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}}\]


\subsection[Supervised]{Decision Tree}
Greedy, top-down, recursive, use
\begin{lstlisting}[mathescape=true]
    DTL(examples, attributes, default):
        if (examples = $\emptyset$): return default
        if ($\forall e \in example, \text{e has the same classification }c$): return c
        if (attributes = $\emptyset$): return mode(examples)

        best = choose_attribute(attributes, examples)
        tree = new decision tree with root test $best$
        for $v_i$ of $best$ do:
            examples_i = $\{e | e \in examples, e.best = v_i\}$
            subtree = DTL(examples, attributes \ best, mode(examples))
            tree.add($v_i$: subtree)
    \end{lstlisting}
\[f: [\text{attribute vector}] \mapsto Boolean\]
Basically nested if-else 

\subsubsection*{Expresssioveness }
Decision trees can express any function of the input attributess. Trivially, Consistent trainign set $\rightarrow$ Consistent decision tree, but unlikely to generalize to new examples\newline
Each row in the truth table is represented as a path in the decison tree 
\subsubsection*{Size of the hypothesis class}
For n boolean attributes, there are n boolean func, n distinct truth tables rach with $2^n$ rows $\rightarrow 2^{2^n}$ decision trees
\subsubsection*{Informativeness}
Ideally we want to sekect an attibute that aplits the examples into all positive or all negative.\newline
Entropy: The higher the more random, thus less informative
\[I(P(v_1)...P(v_k)) = -\sum_{i=1}^{k} P(v_i)log_2P(v_i)\]
For data set contains boolean outputs, 
\[I(P(+), P(-)) = -\frac{p}{p + n}log_2 \frac{p}{p + n} - \frac{n}{p+n}log_2\frac{n}{p+n}\]
where $0 \leq \mathbb{R}_I \leq 1$. However for non-binary variables the entropy can be greater than 1
\subsubsection*{Information gain}
Information gain = entropy of this node - entropy of children nodes
\[IG(A) = I(\frac{p}{p + n}, \frac{n}{p + n}) - remainder(A)\]
\[remainder(A) = \sum_{i=1}^{v}\frac{p_i + n_i}{p + n}I(\frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i})\]
        

\subsubsection*{Decision tree pruning}
\begin{itemize}
    \item By sample size 
    \item by max search depth
\end{itemize}
\subsection{unsupervised}
FInd pattern. No feedback given. e,g, group images by char 
\subsection{Reinforment}
Trial and error, reward given based on observation adn action. eg. chess 










\section{Linear regression}
\subsection{LR}
\begin{itemize}
    \item Data: N data points: $([(feature\_vector, target)])$
\end{itemize}

Regression: given $x \in \mathbb{R}^d$ and no target, 
find a function that predicts the target $y \in \mathbb{R}$ 

the function:
\[f: \mathbb{R}^d \mapsto \mathbb{R}\]

Linear Model:
\[h_w(x) = \sum_{i=1}^{d} w_i x_i = v_w^Tx\]
Where $w_i$ are the parameters or weights and $x_0 = 1$ is a dummy variable (always 1), $w_0$ is the bias.
when d = 1, the model is linear


\subsubsection*{Feature Transformation}
Modify the original features of a dataset to make them more suitable for modeling.
\begin{itemize}
    \item Feature engineering
        \begin{itemize}
            \item Polynomial features: $z = x^k, k$ is the polynomial degree.
            \item log feature: $z = \log(x)$
            \item Exp. feature: $z = e^x$
        \end{itemize}
    \item Feature scale
        \begin{itemize}
            \item Min-max scaling: $z_i = \frac{x_i - min(x_i)}{max(x_i) - min(x-i)}$, scales to [0, 1]
            \item standardization: $z_i = \frac{x_i - \mu_i}{\sigma_i}$, transformed data has mean of 0 and SD of 1
            \item robust scaling (not in syl)
        \end{itemize}
    \item Feature encoding (not in syl)
\end{itemize}

\subsubsection*{Measruing Fit}
Loss function (MSE of the for N samples):
\begin{equation}
    J_{MSE}(w) = \frac{1}{2N}\sum_{i=1}^{N} (h_w(x^{(i)}) - y^{(i)})^2   
\end{equation}
Since polynomial regression can be converted into linear regression, by theorem 7.2, the MSE loss function on linear/poly
nomial regression is convex



\subsection{Normal Equation}
\begin{enumerate}
    \item Take the partial derivative of the linear model
    \begin{equation}
        \frac{\partial(h_w(x^{(i)}))}{\partial w_j} = \frac{\partial(w^Tx^{(i)})}{\partial w_j} = x_j^{(i)}    
    \end{equation}
    \item Take the par. deriv. of each term in the sum, with respect to $w_0$ and $w_1$
    \begin{equation}
        \frac{\partial((h_w(x^{(i)})-y)^2)}{\partial(w_j)} = 2 (h_w(x^{(i)}) - y) x_j^{(i)}  \text{     (chain rule)}
    \end{equation}
    \item hence
    \begin{equation}
        \frac{\partial}{\partial w_j} J_{MSE}(w) = \frac{1}{2N} \frac{\partial}{\partial w_j}\sum_{i=1}^{N}(h_w(x^{(i)})-y)^2 \\
        = \frac{1}{N} \sum_{i=1}^{N}(h_w(x^{(i)}) - y) x_j^{(i)}
    \end{equation}
    As \(h_w(x^{(i)}) = w_0 + w_1(x^{(i)})\) in linear regression,
    \[\frac{\partial }{\partial w_0} J_{MSE}(w)= \frac{1}{N} \sum_{i=1}^{N}((w_0 + w_1 x^{(i)}) - y^{(i)}) \quad {(x_0 = 1)}\]
    \[\frac{\partial }{\partial w_1} J_{MSE}(w)= \frac{1}{N} \sum_{i=1}^{N}((w_0 + w_1 x^{(i)}) - y^{(i)})(x^{(i)})\]
    In generalized form, where $N > 1$, 
    \[\frac{\partial }{\partial w_N} \left[ \frac{1}{2N} \sum_{i=1}^{N} \left(h_w(x^{(i)}) - y^{(i)}\right)^2 \right]= \frac{1}{N} \sum_{i=1}^{N}(h_w(x^{(i)}) - y^{(i)})(x_N^{(i)})\]
\end{enumerate}
Code example:
\begin{lstlisting}[mathescape=true]
def gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250):'''
    Approximate bias and weight that gave the best fitting line.
        Parameters
        ----------
            X (np.ndarray) : (m, n) numpy matrix representing feature matrix
            y (np.ndarray) : (m, 1) numpy matrix representing target values
            lr (float) : Learning rate
            number_of_epochs (int) : Number of gradient descent epochs
        Returns
        -------
            bias (float):
                The bias constant
            weights (np.ndarray):
                A (n, 1) numpy matrix that specifies the weight constants.
            loss (list):
                A list where the i-th element denotes the MSE score at i-th epoch.'''
    bias:number = 0
    weights = np.full((X.shape[1], 1), 0).astype(float)
    loss:List[number] = []
    N:number = X.shape[0]  
    pred = X @ weights + bias       # pred:$\mathbb{R}^{m\times 1}$ = $X:\mathbb{R}^{m\times n} \times w:\mathbb{R}^{n\times 1} $+ bias:number
    for e in range(number_of_epochs): 
        pred = X @ weights + bias
        g_w = (1 / N) * (X.T @ (pred - y)) # $\frac{\partial }{\partial w_1}J_{MSE}(w) = \frac{1}{N} X^T((Xw + bias) - y)$
        g_b = (1 / N) * np.sum(pred - y)   # $\frac{\partial }{\partial w_0}J_{MSE}(w) = \frac{1}{N} \sum_{i=1}^{N} ((Xw + bias) - y)$
        bias -= lr * g_b                   # $w_0 \leftarrow w_0 - \gamma \frac{\partial J_{MSE}(w)}{\partial w_0}$
        weights -= lr * g_w                # $w_1 \leftarrow w_1 - \gamma \frac{\partial J_{MSE}(w)}{\partial w_1}$
        loss.append(mean_squared_error(y, pred))
    return bias, weights, loss  
\end{lstlisting}


\subsubsection*{Normal Equation}
find w that minimizes $J_{MSE}$:
\[\frac{\partial(J_{MSE})}{\partial(w_j)} = \frac{1}{N} \sum_{i = 1}^{N} (w^Tx^{(i)} - y^{(i)})x_j^{(i)} = 0\]
Where \(w = 
\begin{bmatrix}
    w_0 \\w_1 \\\dots \\w_N
\end{bmatrix}\), $x = \begin{bmatrix}
    x_0 \\x_1 \\ \dots \\x_N 
\end{bmatrix}$, $w^Tx$ is just the linear combination of feature vector $w$ on $x$
\[X^T(Xw - Y) = 0\]
Suppose invertible
\[w = (X^TX)^{-1}X^TY\]
\subsection*{Derive Normal equation from MSE}
\noindent Derive:
  $w = (X^T X)^{-1} X^T y$
From:
   $ MSE = \frac{1}{N} \sum_{i=1}^{N}(\hat{y}^{(i)} - y^{(i)})^2 $

\subsubsection*{Proof}
We start with the Mean Squared Error (MSE) function:
\begin{equation}
    MSE = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}^{(i)} - y^{(i)})^2,
\end{equation}
where \( \hat{y}^{(i)} = h_w(x^{(i)}) = w^T x^{(i)} \), with \( w \in \mathbb{R}^{d \times 1} \) and \( y \in \mathbb{R}^{N \times 1} \).
Expanding in terms of matrix notation, let \( A = Xw - y \), where \( X \in \mathbb{R}^{N \times d} \) with rows \( x^{(i)} \), then:
\begin{equation}
    MSE = \frac{1}{N} \sum_{i=1}^{N} (w^T x^{(i)} - y^{(i)})^2 = \frac{1}{N} (A^T A).
\end{equation}
Rewriting:
\begin{equation}
    MSE = \frac{1}{N} (Xw - y)^T (Xw - y),
\end{equation}
which expands further as:
\begin{equation}
    MSE = \frac{1}{N} \left( w^T X^T X w - w^T X^T y - y^T X w + y^T y \right).
\end{equation}
Since \( w^T X^T X w \) and \( y^T X w \) are scalars, we simplify:
\begin{equation}
    MSE = \frac{1}{N} \left( w^T X^T X w - 2 w^T X^T y + y^T y \right).
\end{equation}
To minimize MSE, we take the partial derivative with respect to \( w \):
\begin{equation}
    \frac{\partial}{\partial w} \left[ \frac{1}{N} \left( w^T X^T X w - 2 w^T X^T y + y^T y \right) \right].
\end{equation}
Differentiating term by term:
\begin{equation}
    \frac{\partial}{\partial w} (w^T X^T X w) = 2X^T X w,
\end{equation}
\begin{equation}
    \frac{\partial}{\partial w} (-2 w^T X^T y) = -2 X^T y,
\end{equation}
\begin{equation}
    \frac{\partial}{\partial w} (y^T y) = 0.
\end{equation}
Thus,
\begin{equation}
    \frac{\partial}{\partial w} (MSE) = \frac{1}{N} (2X^T X w - 2X^T y).
\end{equation}
Setting the derivative to zero for minimization:
\begin{equation}
    \frac{2}{N} X^T X w - \frac{2}{N} X^T y = 0.
\end{equation}
\begin{equation}
    X^T X w = X^T y.
\end{equation}
Solving for \( w \), assuming \( X^T X \) is invertible:
\begin{equation}
    w = (X^T X)^{-1} X^T y.
\end{equation}

\subsection{Gradient Descent}

\begin{itemize}
    \item GD Algorithm
    \item Variant: mini-batch, stochastic
    \item Problems and Solutions
\end{itemize}



Computing the paritial derivative to find the minimum is costly, use local search to find a local min

\[w_j \leftarrow w_j - \gamma \frac{\partial(J(w_0, w_1\dots))}{\partial(w_j)}\]
Where $\gamma$ is the learning rate. \newline
Repeat until termination criterion is satisfied


\subsubsection*{Convexity}
\begin{theorem}
    A convex function has one singular global minimum.
\end{theorem}

\begin{theorem}
    MSE loss function is convex for linear regression
\end{theorem}

\subsubsection*{Variants}
\begin{itemize}
    \item Batch: take all training samples
    \item Mini-batch: random subset
    \item Stochastic: select one random sample
\end{itemize}


\section*{Linear regression with Regularization}
Knowing the hypothesis 
\[h_w(x): w^Tx\]
Penalize weights in the loss fucntion
\[J_{reg}(w) = J(w) + \lambda P(w)\]
Optimization goal: $\min(J_{reg}(w))$\\
Penalty functions
\begin{enumerate}
    \item L1 penalty (Laso Regression in Linear regression): $P(w) = \sum_{j=0}^{d}|w_j|$
    \item L2 penalty (Ridge regression in linear regression): $P(w) = \sum_{j=0}^{d}\frac{1}{2}w^2_j$
\end{enumerate}
\subsubsection*{Gradient descent of regularized loss function}
\begin{equation}
    \frac{\partial J_{reg}(w)}{\partial w} = \frac{\partial J(w)}{\partial w} + \frac{\partial \lambda P(w)}{\partial w}
\end{equation}
Regularized cost function
\[J(w) = \frac{1}{2m}\left[\sum_{i = 1}^{m}(h_w(x^{(i)})- y{(i)}) + \lambda\sum_{i=1}^{n} w_i^2\right]\]
Where \\
$\sum_{i = 1}^{m}(h_w(x^{(i)})- y{(i)})$ fits the data "well", where
$ \lambda\sum_{i=1}^{m} w_i^2$ is the regularization parameter, that avoid "over-fitting"

\[w = (X^TX)^{-1}X^TY\]

\[w = (X^TX + \lambda \begin{bmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots  & 1
\end{bmatrix}^{m\times m}) ^{-1} X^TY\]\\
$\forall \lambda > 0, X^TX + \lambda I$ is invertible\\
As \(w = 
\begin{bmatrix}
    w_0 \\w_1 \\\dots \\w_n
\end{bmatrix}\), here in the regularized cost function, m indicates the size of the training set, while n indicates the size of the feature vector
large $\lambda \rightarrow$ underfitting; $\lambda = 0 \rightarrow$ overfitting 
%=============================================================

\section{Logistic Regression}
\subsection*{Classification Task}
\[x \in \mathbb{R}^d \mapsto void\]
Find a function thats 
\[y \mapsto \{0, 1\} \text{, for each } x\]
\subsection*{Sigmoid}
\[\sigma(x) = \frac{1}{1 + e^{-x}}\]
\[\sigma'(x) = \frac{-(1 + e^{-x})'}{(1 + e^{-x})^2} = \frac{e^{-x}}{1 + 2e^{-x} + e^{-2x}}\]

\subsection*{$\sigma(f(x))$}
\[\sigma(x^2) = \frac{1}{1 + e^{-x^2}}\]

\subsection*{Decision Boundary}
A surface that separates the different classes in the featyre space \\
The intersection betweeen the hypothesis function and the decision threshold \\

\subsection*{Binary Cross Entropy}
\[BCE(y, \hat{y}) = -ylog(\hat{y}) - (1 - y)log(1 - \hat{y})\]
\[J_{BCE}(w) = \frac{1}{N} \sum_{i=1}^{N} BCE(y^{(i)}, h_w(x^{(i)}))\]
$\sigma(x) = \frac{1}{1 + e^{-x}}$ is Not convex, but
$-log(\sigma(x))$ is convex\\
Hypothesis function
\[h_w(x) = \sigma(w_0, w_1 x_1 + w_2 x_2) \]
Weight Update
\[w_j \leftarrow w_j - \gamma \frac{\partial J_{BCE}(w_0, w_1, \dots)}{\partial w_j}\]
Loss function derivative
\[\frac{\partial J_{BCE}(w_j)}{\partial w} = \frac{\partial}{\partial w_j} \frac{1}{N} \sum_{i=1}^{N} BCE(y^{(i)}, h_w(x^{(i)})) = \frac{1}{N} \sum_{i=1}^{N} (h_w(x^{(i)}) - y^{(i)})x_j^{(i)}\]
\subsection{Logistic regression cost function}
\begin{equation}
    \begin{system}
    -log(h_w(x)), \text{ if y = 1}\\
    -log(1 - h_w(x)), \text{if y = 0}
    \end{system}
\end{equation}


\subsection*{Multiclass classification}
\subsubsection*{One-to-one}
\[\{0, 1, ... C\} \times \{0, 1, ... C\} \rightarrow (C^2 - C) \text{ BCE}\]
One-vs-One is a technique where a separate binary classifier is $\mathcal{C}$ trained for every pair of classes. For $\mathcal{C}$ classes, this results in $\frac{\mathcal{C}(\mathcal{C} - 1)}{2}$ classifiers. Each classifier distinguishes between two specific classes.
\subsubsection*{One-vs-Rest}
\[\{0, 1, ... C\}.map(c \rightarrow \{0, 1, ... C\} \textbackslash \{c\})\]
One-vs-Rest is a technique where a separate binary classifier is trained for each class, treating all other classes as a single combined class. For $\mathcal{C}$ classes, this results in $\mathcal{C}$ classifiers.


\subsection*{Generalization}
\begin{itemize}
    \item In supervised learning, and machine learning in general, the model's performance on unseen data is all we care about. This ability to perform well on new, unseen data is known as the model's generalization capability. 
    \item Measuring a model's error is a common practice to quantify the performance of the model. This error, when evaluated on unseen data, is known as the generalization error.
    \item There are two factors that affect generalization:
    \begin{itemize}
        \item Dataset quality 
        \begin{itemize}
            \item Relevance: Dataset should contain relevant data, i.e., features that are relevant for solving the problem.
            \item Noise: Dataset may contain noise (irrelevant or incorrect data), which can hinder the model's learning process and reduce generalization.
            \item Balance (for classification): Balanced datasets ensure that all classes are adequately represented, helping the model learn to generalize well across different classes.
        \end{itemize}
        \item Data quantity
        \begin{itemize}
            \item In general, having more data typically leads to better model performance, provided that the model is expressive enough to accurately capture the underlying patterns in the data
            \item Extreme case: if the dataset contains every possible data point, the model would no longer need to "guess" or make predictions. Instead, it would only need to simply memorize all the data!
        \end{itemize}
        \item Model complexity
        \begin{itemize}
            \item Refers to the size and expressiveness of the hypothesis class.
            \item Indicates how intricate the relationships between input and output variables that the model can capture are.
            \item Higher model complexity allows for more sophisticated modeling of input-output relationships
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection*{Hyperparameter}
Hyperparameters are settings that control the behavior of the training algorithm and model but are not learned from the data. They need to be set before the training process begins. Such as 
\begin{itemize}
    \item Learning rate
    \item Feature transformations
    \item Batch size and iterations in mini-batch gradient descent
\end{itemize}
\subsection*{Hyperparameter Tuning}
Hyperparameter tuning is the process of hyperparameters optimizing the of a machine learning model to improve its performance. It is also known as hyperparameter search.
\subsubsection*{Objective} 
The goal is to find the best combination of hyperparameters that maximize the model's performance.




\end{document}


